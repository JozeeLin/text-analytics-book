{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于依存关系的分析\n",
    "在基于依存关系的分析中,我们会**使用依存语法**来**分析和推断**语句中**每个标识在结构上和语义上的关系**..\n",
    "\n",
    "基于依存关系的语法可以帮助我们**使用依存标签标注句子**.\n",
    "\n",
    "依存标签是标识之间的**一对一的映射**,表示标识之间的依存关系.\n",
    "\n",
    "基于依存语法的**分析树是一个有标签且有方向的树或图**,可以更加精确地表示语句.\n",
    "\n",
    "分析树中的节点始终是词汇类型的标识,有标签的边表示起始点及其从属项(依赖起始点的标识)的依存关系.\n",
    "\n",
    "有向边上的标签表示依存关系中的语法角色.\n",
    "\n",
    "在本节中,我们将介绍一些基于依存语法的分析,使我们进一步了解文本标识之间的句法和语义."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依存关系分析器推荐\n",
    "接下来我们使用几个库生成基于依存关系的分析树,并对我们的例句进行测试.\n",
    "\n",
    "首先,我们将使用spacy库来分析我们的例句,生成所有标识及其依存关系."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]<---The[det]--->[]\n",
      "--------\n",
      "[]<---brown[amod]--->[]\n",
      "--------\n",
      "[u'The', u'brown']<---fox[nsubj]--->[]\n",
      "--------\n",
      "[u'fox']<---is[ROOT]--->[u'quick', u'and', u'jumping']\n",
      "--------\n",
      "[]<---quick[acomp]--->[]\n",
      "--------\n",
      "[]<---and[cc]--->[]\n",
      "--------\n",
      "[]<---he[nsubj]--->[]\n",
      "--------\n",
      "[]<---is[aux]--->[]\n",
      "--------\n",
      "[u'he', u'is']<---jumping[conj]--->[u'over']\n",
      "--------\n",
      "[]<---over[prep]--->[u'dog']\n",
      "--------\n",
      "[]<---the[det]--->[]\n",
      "--------\n",
      "[]<---lazy[amod]--->[]\n",
      "--------\n",
      "[u'the', u'lazy']<---dog[pobj]--->[]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "#from spacy.lang.en import English\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "parsed_sent = nlp(unicode(sentence))\n",
    "\n",
    "#parser = English()\n",
    "#parsed_sent = parser(unicode(sentence))\n",
    "\n",
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "\n",
    "#for token in parsed_sent:\n",
    "#    print dependency_pattern.format(word=token.orth_,\n",
    "#                                   w_type=token.dep_,\n",
    "#                                   left=[t.orth_ for t in token.lefts],\n",
    "#                                   right=[t.orth_ for t in token.rights])\n",
    "    \n",
    "for token in parsed_sent:\n",
    "    print dependency_pattern.format(word=token.orth_,\n",
    "                                   w_type=token.dep_,\n",
    "                                   left=[t.orth_ for t in token.lefts],\n",
    "                                   right=[t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的输出显示了所有的标识及其依存关系类型,左箭头指向其左侧的依存项,右箭头指向其右侧的依存项.\n",
    "\n",
    "关于每个依存标签所代表的含义,请参阅第一章.\n",
    "\n",
    "接下来,我们将使用nltk和斯坦福分析器为例句生成依存关系树,代码段所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"500pt\" height=\"392pt\"\n",
       " viewBox=\"0.00 0.00 500.00 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 496,-388 496,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\"><title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (quick)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.5,-347.799C178.5,-336.163 178.5,-320.548 178.5,-307.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182,-307.175 178.5,-297.175 175,-307.175 182,-307.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\"><title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"70.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M150.12,-260.902C141.668,-255.468 132.525,-249.241 124.5,-243 114.257,-235.034 103.642,-225.519 94.5625,-216.937\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.9768,-214.403 87.3388,-210.004 92.1294,-219.453 96.9768,-214.403\"/>\n",
       "<text text-anchor=\"middle\" x=\"139.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\"><title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"142.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (is)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.215,-260.799C166.189,-248.932 159.411,-232.928 153.702,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.897,-218.019 149.774,-210.175 150.451,-220.749 156.897,-218.019\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (and)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.988,-260.799C191.153,-248.932 198.12,-232.928 203.987,-219.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.242,-220.741 208.024,-210.175 200.823,-217.947 207.242,-220.741\"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">cc</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\"><title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">9 (jumping)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M203.593,-260.799C222.418,-247.895 248.378,-230.099 268.98,-215.978\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271.175,-218.716 277.444,-210.175 267.217,-212.943 271.175,-218.716\"/>\n",
       "<text text-anchor=\"middle\" x=\"260.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">conj</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\"><title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"110.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (brown)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62.0007,-173.799C56.1371,-161.932 48.2291,-145.928 41.5688,-132.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44.5545,-130.59 36.9867,-123.175 38.2788,-133.691 44.5545,-130.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"61\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M78.5946,-173.799C84.179,-161.932 91.7104,-145.928 98.0535,-132.449\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.326,-133.714 102.417,-123.175 94.9925,-130.733 101.326,-133.714\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\"><title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"230.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (he)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M285.124,-173.944C279.656,-168.359 273.693,-162.037 268.5,-156 261.745,-148.148 254.771,-139.238 248.67,-131.136\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.447,-129.005 242.674,-123.065 245.828,-133.18 251.447,-129.005\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\"><title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">8 (is)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.5,-173.799C302.5,-162.163 302.5,-146.548 302.5,-133.237\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"306,-133.175 302.5,-123.175 299,-133.175 306,-133.175\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">aux</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node11\" class=\"node\"><title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"379.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">13 (dog)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;13 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>9&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M318.082,-173.799C329.25,-161.471 344.462,-144.679 356.952,-130.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"359.821,-132.936 363.941,-123.175 354.633,-128.237 359.821,-132.936\"/>\n",
       "<text text-anchor=\"middle\" x=\"361.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\"><title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"298.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">10 (over)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>13&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.108,-86.799C351.251,-74.3561 335.059,-57.3644 321.851,-43.5044\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"324.3,-41.0003 314.867,-36.1754 319.232,-45.8294 324.3,-41.0003\"/>\n",
       "<text text-anchor=\"middle\" x=\"355.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node13\" class=\"node\"><title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"379.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">11 (the)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>13&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M379.5,-86.799C379.5,-75.1626 379.5,-59.5479 379.5,-46.2368\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"383,-46.1754 379.5,-36.1754 376,-46.1755 383,-46.1754\"/>\n",
       "<text text-anchor=\"middle\" x=\"388\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\"><title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"459.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">12 (lazy)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M395.689,-86.799C407.4,-74.3561 423.392,-57.3644 436.437,-43.5044\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"439.03,-45.8563 443.335,-36.1754 433.932,-41.0587 439.03,-45.8563\"/>\n",
       "<text text-anchor=\"middle\" x=\"439\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 14 nodes>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "java_path = r'/usr/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "sdp = StanfordDependencyParser(path_to_jar='/home/parallels/stanford-nlp/stanford-parser-full-2015-04-20/stanford-parser.jar', \n",
    "                               path_to_models_jar='/home/parallels/stanford-nlp/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "#generate annotated dependency parse tree\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(quick (fox The brown) is and (jumping he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "#generate dependency triples\n",
    "dep_tree = [parse.tree() for parse in result][0]\n",
    "print dep_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize simple dependency parse tree\n",
    "dep_tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的输出结果展示了我们如何轻松地为例句生成依存分析树,并分析和理解标识间的关系.\n",
    "\n",
    "斯坦福分析器是十分强大且稳定的,它能够很好地与nltk集成."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立自己的依存关系分析器\n",
    "从头开始构建自己的依存关系分析器并不容易,因为:\n",
    "- 需要大量的,充足的数据\n",
    "- 仅仅按照语法产生式规则来检查并不总是能够很好地评估分析器效果\n",
    "\n",
    "1. 首先利用nltk的DependencyGrammar类从用户输入语法获得产生式规则.\n",
    "2. 然后使用ProjectiveDependencyParser(一个映射式的,基于产生式规则的依存关系分析器)来执行基于依存关系的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 12 productions\n",
      "  'fox' -> 'The'\n",
      "  'fox' -> 'brown'\n",
      "  'quick' -> 'fox'\n",
      "  'quick' -> 'is'\n",
      "  'quick' -> 'and'\n",
      "  'quick' -> 'jumping'\n",
      "  'jumping' -> 'he'\n",
      "  'jumping' -> 'is'\n",
      "  'jumping' -> 'dog'\n",
      "  'dog' -> 'over'\n",
      "  'dog' -> 'the'\n",
      "  'dog' -> 'lazy'\n",
      "(quick (fox The brown) is and (jumping he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "dependency_rules = \"\"\"\n",
    "'fox' -> 'The'|'brown'\n",
    "'quick' -> 'fox'|'is'|'and'|'jumping'\n",
    "'jumping' -> 'he' | 'is' | 'dog'\n",
    "'dog' -> 'over' | 'the' | 'lazy'\n",
    "\"\"\"\n",
    "\n",
    "dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_rules)\n",
    "print dependency_grammar\n",
    "\n",
    "#build dependency parser\n",
    "dp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "\n",
    "#parse our sample sentence\n",
    "res = [item for item in dp.parse(tokens)]\n",
    "tree = res[0]\n",
    "\n",
    "#print dependency parse tree\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(quick (fox The brown) is and (jumping he is (dog over the lazy)))\n"
     ]
    }
   ],
   "source": [
    "#build dependency parser\n",
    "dp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "#parse our sample sentence\n",
    "res = [item for item in dp.parse(tokens)]\n",
    "tree = res[0]\n",
    "\n",
    "#print dependency parse tree\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f分析树可视化树形结构,并将其与上一个树形结构进行比较.\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于成分结构的分析\n",
    "基于成分结构的语法常用来分析和确定语句的组成成分.\n",
    "\n",
    "这种语法的另一个重要用途是**找出这些组成成分的内部结构以及它们之间的关系**.\n",
    "\n",
    "更多信息请参阅1.3.4节下的\"成分语法\"内容.\n",
    "\n",
    "基于成分结构的语法可以帮助我们将句子分解成各种成分.\n",
    "\n",
    "然后,将这些成分分解成更细的细分项,并且重复这个过程直至将各种成分分解成独立的标识或单词.\n",
    "\n",
    "一般而言,一个与上下文语境无关的语法(CFG)或短语结构语法就足以完成上述操作.\n",
    "\n",
    "分析器是为语法赋予生命的东西,也可以是语法的程序语言解释.\n",
    "\n",
    "目前,有各种类型的分析算法,包括如下几类:\n",
    "\n",
    "- 递归下降解析(Recursive Descent parsing)\n",
    "- 移位归约解析(Shift Reduce parsing)\n",
    "- 图表解析(Chart parsing)\n",
    "- 自下而上解析(Bottom-up parsing)\n",
    "- 自上而下解析(Top-down parsing)\n",
    "- PCFG解析(PCFG parsing)\n",
    "关于以上的分析算法,更加详细信息请参阅http://www.nltk.org/book/ch08.html\n",
    "\n",
    "**递归下降解析**通常遵循自上而下的解析方法,它从输入语句中读取标识,然后尝试将其与语法产生式规则中的终结符进行匹配.\n",
    "\n",
    "它始终超前一个标识,并在每次获得匹配时,将输入读取指针前移.\n",
    "\n",
    "**移位归约解析**遵循自下而上的解析方法,它找出与语法产生式规则右侧一致的标识序列(单词或短语),然后用该规则左侧的标识替换它.\n",
    "\n",
    "这个过程一直持续,直到整个句子只剩下形成分析树的必要项.\n",
    "\n",
    "**图表解析**采用动态规划,它存储中间结果,并在需要是重新使用这些结果,以获得显著的效能提升.这种情况下,图表分析器存储部分解决方案,\n",
    "\n",
    "并在需要时查找它们以获得完整的解决方案."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 成分结构分析器推荐\n",
    "使用nltk的StanfordParser来生成分析树."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (NP\n",
      "    (S\n",
      "      (S\n",
      "        (NP (DT The) (JJ brown) (NN fox))\n",
      "        (VP (VBZ is) (ADJP (JJ quick))))\n",
      "      (CC and)\n",
      "      (S\n",
      "        (NP (PRP he))\n",
      "        (VP\n",
      "          (VBZ is)\n",
      "          (VP\n",
      "            (VBG jumping)\n",
      "            (PP (IN over) (NP (DT the) (JJ lazy) (NN dog)))))))))\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "scp = StanfordParser(path_to_jar='/home/parallels/stanford-nlp/stanford-parser-full-2015-04-20/stanford-parser.jar', \n",
    "                path_to_models_jar='/home/parallels/stanford-nlp/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "#get parse tree\n",
    "result = list(scp.raw_parse(sentence))\n",
    "\n",
    "#print the constituency parse tree\n",
    "print result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize constituency parse tree\n",
    "result[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码段向我们展示了如何为句子构建基于成分语法的分析树."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建自己的成分结构分析器\n",
    "构建自己的成分结构分析器有各种各样的方法,包括创建CFG产生式规则,然后使用该语法规则构建分析器等.\n",
    "\n",
    "想要构建自己的CFG,你可以使用nltk.CFG.fromstring函数来输入产生式规则.\n",
    "\n",
    "然后再使用ChartParser或RecursiveDescentParser分析器.\n",
    "\n",
    "我们将会考虑构建一个扩展性良好且运行高效的成分结构分析器.\n",
    "\n",
    "常规CFG分析器(如图表分析器,递归下降分析器)的问题是解析语句时很容易被大量的解析工作量所压垮,导致运行速度非常缓慢\n",
    "\n",
    "PCFG(概率上下文无关语法)这样的加权语法和像维特比分析器这样的概率分析器在运行中被证明更有效的地方.\n",
    "\n",
    "PCFG是一种上下文无关的语法,它将每个产生式规则与一个概率值相关联.\n",
    "\n",
    "从PCFG产生一个分析树的概率是每一个产生式规则概率的乘积.\n",
    "\n",
    "---------------------------------------\n",
    "接下来使用nltk的ViterbiParser来训练treebank语料库中的分析器,treebank语料库为每个句子提供了带注释的分析树.\n",
    "\n",
    "一个自下而上的PCFG分析器,它使用动态规划来查找每个步骤中最有可能的解析结果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "#get training data\n",
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "#view a sample training sentence\n",
    "print training_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "    set(production for sent in training_set for production in sent.productions())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VBZ -> 'cites',\n",
       " VBD -> 'spurned',\n",
       " PRN -> , ADVP-TMP ,,\n",
       " NNP -> 'ACCOUNT',\n",
       " JJ -> '36-day',\n",
       " NP-SBJ-2 -> NN,\n",
       " JJ -> 'unpublished',\n",
       " NP-SBJ-1 -> NNP,\n",
       " JJ -> 'elusive',\n",
       " NNS -> 'Lids']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_productions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word, tag in treebank.tagged_words():\n",
    "    t = nltk.Tree.fromstring(\"(\"+tag+\" \"+word+\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "        \n",
    "#build the PCFG based grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), treebank_productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9da829e6bc79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#get parse tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviterbi_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/parallels/anaconda2/lib/python2.7/site-packages/nltk/parse/viterbi.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The most likely constituent table.  This table specifies the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/parallels/anaconda2/lib/python2.7/site-packages/nltk/grammar.pyc\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 648\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\"."
     ]
    }
   ],
   "source": [
    "#通过语法训练创建自己的分析器,然后使用例句对分析器进行评估\n",
    "\n",
    "#build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "#get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "#get parse tree\n",
    "result = list(viterbi_parser.parse(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码报错的原因:例句中的一些单词不包含在基于treebank的语法中,因为这些单词并不在我们的treebank语料库中.\n",
    "\n",
    "由于该语法使用POS标签和短语标签来构建基于训练数据的分析树,我们将在语法中为例句添加标识和POS标签,然后重新构建分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import tag as pos_tagger\n",
    "tagged_sent = pos_tagger(sentence)\n",
    "\n",
    "print tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8fb0bf61ad7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#get parse tree for sample sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviterbi_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print the constituency parse tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/parallels/anaconda2/lib/python2.7/site-packages/nltk/parse/viterbi.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The most likely constituent table.  This table specifies the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/parallels/anaconda2/lib/python2.7/site-packages/nltk/grammar.pyc\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 648\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\"."
     ]
    }
   ],
   "source": [
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+tag+\" \"+word+\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "        \n",
    "#rebuild grammar\n",
    "treebank_parser = nltk.grammar.induce_pcfg(Nonterminal('S'), treebank_productions)\n",
    "\n",
    "#rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "#get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "#print the constituency parse tree\n",
    "print result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
