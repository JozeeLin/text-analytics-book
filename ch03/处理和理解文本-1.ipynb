{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现数值格式的特征输入,你需要清洗,规范化和预处理初始文本数据.通常,文本语料库和原始文本的数据格式既非准确的,也非规范的.文本处理,涉及使用各种技术将原始文本转换成定义良好的语言成分序列,这些序列具有标准的结构和标记.\n",
    "\n",
    "以下是将在本章中探讨的一些主流文本预处理技术:\n",
    "- 切分(tokenization)\n",
    "- 标注(tagging)\n",
    "- 分块(chunking)\n",
    "- 词干提取(stemming)\n",
    "- 词性还原(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本切分\n",
    "标识(token)是具有一定的句法语义且独立的最小文本成分.\n",
    "\n",
    "一段文本或一个文本文件具有几个组成部分,包括可以进一步细分为从句,短语和单词的语句.\n",
    "\n",
    "最流行的文本切分技术包括句子切分和词语切分,用于将文本语料库分解成句子,并将每个句子分解成单词.\n",
    "\n",
    "文本切分可以定义为将文本数据分解或拆分为具有更小且有意义的成分的过程.\n",
    "\n",
    "### 句子切分\n",
    "句子切分(sentence tokenization)是将文本语料库分解成句子的过程.\n",
    "\n",
    "执行句子切分有多种技术,基本技术包括在句子之间寻找特定的分隔符.\n",
    "\n",
    "我们将使用NLTK框架进行切分,该框架提供用于执行句子切分的各种接口.我们将主要关注一下句子切分器:\n",
    "- sent_tokenize\n",
    "- PunkSentenceTokenizer\n",
    "- RegexpTokenizer\n",
    "- 预先训练的句子切分模型\n",
    "\n",
    "将文本分割成句子之前,需要一些测试该操作的文本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = '''we will discuss briefly about the basic syntax, structure and design philosophies, \n",
    "                There is a defined hierarchical syntax for Python code which you should remember when writing code!\n",
    "                Python is a really powerful programming language!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n"
     ]
    }
   ],
   "source": [
    "print len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "print alice[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.sent_tokenize函数是nltk推荐的模型的句子切分函数.它内部使用一个PunktSentenceTokenizer类的实例."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 2\n",
      "Sample text sentences :-\n",
      "['we will discuss briefly about the basic syntax, structure and design philosophies, \\n                There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[u\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " u\"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\",\n",
      " u'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.',\n",
      " u\"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " u'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "#以下代码段展示了该函数在实例文本中的基本用法:\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print 'Total sentences in sample_text:', len(sample_sentences)\n",
    "print 'Sample text sentences :-'\n",
    "pprint(sample_sentences)\n",
    "print '\\nTotal sentences in alice:', len(alice_sentences)\n",
    "print 'First 5 sentences in alice:-'\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对德语文本进行句子切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import europarl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "#文本长度\n",
    "print len(german_text)\n",
    "#文本的前100个字符\n",
    "print german_text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "#使用默认的sent_tokenize切分器\n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "#从nltk源加载的预训练的德语切分器\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "#德语切分器也是属于PunktSentenceTokenizer类型\n",
    "print type(german_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的语句输出可以看出german_tokenizer是PunktSentenceTokenizer的一个实例,它专门用来处理德语.\n",
    "\n",
    "接下来,对比使用默认切分器和使用德语切分器的效果有什么不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "print german_sentences_def == german_sentences\n",
    "for sent in german_sentences[0:5]:\n",
    "    print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we will discuss briefly about the basic syntax, structure and design philosophies, \\n                There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "#使用默认的PunktSentenceTokenizer类也能很方便的实现句子切分.\n",
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用正则表达式的模式来切分句子:RegexpTokenizer类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we will discuss briefly about the basic syntax, structure and design philosophies, \\n                There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
      " '                Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词语切分\n",
    "词语切分(word tokenization)是将句子分解或分割成其组成单词的过程.\n",
    "\n",
    "句子是单词的集合,通过词语切分,在本质上即是将一个句子分割成单词列表,该单词列表又可以重建句子.\n",
    "\n",
    "词语切分在很多过程中都是非常重要的,特别是在文本清洗和规范化时,诸如词干提取和词形还原这类基于词干,标识信息的操作会子啊每个单词上实施.\n",
    "\n",
    "nltk的主流词语切分接口:\n",
    "- word_tokenize\n",
    "- TreebankWordTokenizer\n",
    "- RegexpTokenizer\n",
    "- 从RegexpTokenizer继承的切分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.word_tokenize函数是nltk默认并推荐的词语切分器.该切分器实际上是TreebankWordTokenizer类的一个实例或对象,并且是该核心类的一个封装.\n",
    "\n",
    "TreebankWordTokenizer基于Penn Treebank,并使用各种正则表达式来分割文本.该切分器的一些主要功能包括:\n",
    "\n",
    "- 分割和分离出现在句子末尾的句点\n",
    "- 分割和分离空格前的逗号和单引号\n",
    "- 将大多数标点符号分割成独立标识\n",
    "- 分割常规的缩写词--例如将'don't'分割成'do'和'n't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下代码使用正则表达式执行词语切**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+' #定义正则表达式\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+' #定义正则表达式\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print word_indices\n",
    "print [sentence[start:end] for start, end in word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "#使用r'\\w+|[^\\w\\s]+'模式将句子切分成独立的字母和非字母标识\n",
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "#whitespacetokenizer基于诸如缩进符,换行符及空格的空白字符将句子分割成单词\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本规范化\n",
    "- 文本清洗\n",
    "- 大小写转换\n",
    "- 词语校正\n",
    "- 停用词删除\n",
    "- 词干提取\n",
    "- 词形还原\n",
    "\n",
    "文本规范化通常也称为文本清洗或转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "corpus = [\"the brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "         \"@@You'll (learn) a **lot** in the book.Python is  an amazing language!@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本清洗\n",
    "对于清洗html之类的数据源中提取有意义的文本,可以使用nltk的clean_html()函数,或者BeautifulSoup库来解析HTML数据,或者正则表达式,xpath和lxml库来解析XML数据."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本切分\n",
    "文本切分和删除多余字符的顺序取决于你要解决的问题和你正在处理的数据."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    '''\n",
    "    接收文本数据,从中提取句子,最后将每个句子划分成标识.\n",
    "    这些标识可以是单词,特殊字符或标点符号.\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '**lot**',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book.Python',\n",
      "   'is',\n",
      "   'an',\n",
      "   'amazing',\n",
      "   'language',\n",
      "   '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除特殊字符\n",
    "文本规范化中的一个重要任务是删除多余和特殊的字符,诸如特殊符号或标点符号.\n",
    "\n",
    "删除特殊字符的原因是分析文本并提取基于NLP和机器学习的特征或信息时,标点符号或特殊字符往往没有多大的意义."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    '''\n",
    "    string.punctuation由所有可能的特殊字符/符号组成,并从中创建一个正则表达式模式.\n",
    "    使用正则表达式sub算法删除特殊字符\n",
    "    filter函数删除空标识\n",
    "    '''\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) #哇,还可以这么写\n",
    "    filtered_tokens = filter(None, [pattern.sub('',token) for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_list_1 = [filter(None, [remove_characters_after_tokenization(tokens) for tokens in sentence_tokens]) \n",
    "                   for sentence_tokens in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race']], [['Hey', 'that', 's', 'a', 'great', 'deal'], ['I', 'just', 'bought', 'a', 'phone', 'for', '199']], [['You', 'll', 'learn', 'a', 'lot', 'in', 'the', 'bookPython', 'is', 'an', 'amazing', 'language']]]\n"
     ]
    }
   ],
   "source": [
    "print filtered_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    '''\n",
    "    在文本切分之前删除特殊字符\n",
    "    '''\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        #保留撇号和句号\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN =  r'[^a-zA-Z0-9 ]'\n",
    "        filtered_sentence = re.sub(PATTERN,r'',sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the bookPython is  an amazing language']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print filtered_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"the brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book.Python is  an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
    "print cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展缩写词\n",
    "缩写词是词或音节的缩短形式.举例说明:\"is not\"缩写成\"isn't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入缩写词与原始形式的对应关系\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "                                if contraction_mapping.get(match) \\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expaned_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'you will learn a lot in the book.Python is  an amazing language!']\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in cleaned_corpus]\n",
    "print expanded_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 大小写转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n"
     ]
    }
   ],
   "source": [
    "print corpus[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "print corpus[0].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除停用词\n",
    "停用词是指没有或只有极小意义的词语.\n",
    "\n",
    "通常在处理过程中将它们从文本中删除,以保留具有最大意义及语境的词语."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['brown', 'fox', 'quick', 'could', 'win', 'race']], [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']], [['learn', 'lot', 'book.Python', 'amazing', 'language', '!']]]\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus]\n",
    "filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]\n",
    "print filtered_list_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以看到第一句的not,no这样的否定词也被删除掉,通常这些词语应当被保留,以便于在诸如情绪分析等应用中句子语意不会失真**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词语校正\n",
    "文本规范化面临的主要挑战之一是文本中存在不正确的单词.\n",
    "- 校正重复字符,比如'finalllllyyyyyy'校正成'finally'\n",
    "- 校正拼写错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Step: 4 Word: finaly\n",
      "Final word: finaly\n"
     ]
    }
   ],
   "source": [
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "\n",
    "while True:\n",
    "    #remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    if new_word != old_word:\n",
    "        print 'Step: {} Word: {}'.format(step, new_word)\n",
    "        step += 1 #update step\n",
    "        #update old word to last substituted state\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print 'Final word:', new_word\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**上面的词语校正最终结果不是正确的,接下来我们引入Wordnet对每次校正的结果进行语义校正,如果当前词语是有效词语就结束词语校正过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Final correct word: finally\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    #语义检查\n",
    "    if wordnet.synsets(old_word):\n",
    "        print 'Final correct word:', old_word\n",
    "        break\n",
    "    #remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    if new_word != old_word:\n",
    "        print 'Step: {} Word: {}'.format(step, new_word)\n",
    "        step += 1\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print 'Final word:', new_word\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将上面的处理逻辑组织成函数的形式\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word!=old_word else new_word\n",
    "    \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'schooool', 'is', 'realllllyyy', 'amaaaazingggg']\n",
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = 'My schooool is realllllyyy amaaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "print sample_sentence_tokens\n",
    "\n",
    "print remove_repeated_characters(sample_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 校正拼写错误\n",
    "校正算法更多请参考:http://norving.com/spell-correct.html\n",
    "\n",
    "我们的主要目标是,给定一个单词,找出这个单词最有可能的正确形式.\n",
    "\n",
    "我们遵循的方法是:生成一系列类似输入词的候选词,并从该集合中选择最有可能的单词作为正确的单词.\n",
    "\n",
    "根据编辑距离从候选词中选出一个最终的结果\n",
    "\n",
    "最常用单词列表下载地址:http://norving.com/big.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 80030), ('of', 40025), ('and', 38313), ('to', 28766), ('in', 22050), ('a', 21155), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "def tokens(text):\n",
    "    '''\n",
    "    Get all words from the corpus\n",
    "    '''\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "WORDS = tokens(file('big.txt').read())\n",
    "WORDS_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "#top 10 words in the corpus\n",
    "print WORDS_COUNTS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义三个函数分别计算出与输入单词的编辑距离为0,1,2的单词组.\n",
    "\n",
    "def edits0(word):\n",
    "    '''\n",
    "    编辑距离为0\n",
    "    '''\n",
    "    return {word}\n",
    "\n",
    "def edits1(word):\n",
    "    '''\n",
    "    编辑距离为1\n",
    "    '''\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    \n",
    "    pairs = splits(word)\n",
    "    deletes = [a+b[1:] for (a,b) in pairs if b] #删除,结果字符串长度减少1\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a,b) in pairs if len(b)>1] #相邻位置调换\n",
    "    replaces = [a+c+b[1:] for (a,b) in pairs for c in alphabet if b] #分别替换各个位置\n",
    "    inserts = [a+c+b for (a,b) in pairs for c in alphabet]  #插入一个字符,结果字符串长度增加1\n",
    "    return set(deletes+transposes+replaces+inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    '''\n",
    "    编辑距离为2\n",
    "    '''\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据单词是否存在与词汇词典WORD_COUNTS中,从edit函数得出的候选词组中返回一个单词子集.\n",
    "def known(words):\n",
    "    '''\n",
    "    从候选词组中获得一个有效单词列表\n",
    "    '''\n",
    "    return {w for w in words if w in WORDS_COUNTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fianlly'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#基于与输入单词之间的编辑距离,给出最有可能的候选词\n",
    "word = 'fianlly'\n",
    "edits0(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits0(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'afianlly',\n",
       " 'aianlly',\n",
       " 'bfianlly',\n",
       " 'bianlly',\n",
       " 'cfianlly',\n",
       " 'cianlly',\n",
       " 'dfianlly',\n",
       " 'dianlly',\n",
       " 'efianlly',\n",
       " 'eianlly',\n",
       " 'faanlly',\n",
       " 'faianlly',\n",
       " 'fainlly',\n",
       " 'fanlly',\n",
       " 'fbanlly',\n",
       " 'fbianlly',\n",
       " 'fcanlly',\n",
       " 'fcianlly',\n",
       " 'fdanlly',\n",
       " 'fdianlly',\n",
       " 'feanlly',\n",
       " 'feianlly',\n",
       " 'ffanlly',\n",
       " 'ffianlly',\n",
       " 'fganlly',\n",
       " 'fgianlly',\n",
       " 'fhanlly',\n",
       " 'fhianlly',\n",
       " 'fiaally',\n",
       " 'fiaanlly',\n",
       " 'fiablly',\n",
       " 'fiabnlly',\n",
       " 'fiaclly',\n",
       " 'fiacnlly',\n",
       " 'fiadlly',\n",
       " 'fiadnlly',\n",
       " 'fiaelly',\n",
       " 'fiaenlly',\n",
       " 'fiaflly',\n",
       " 'fiafnlly',\n",
       " 'fiaglly',\n",
       " 'fiagnlly',\n",
       " 'fiahlly',\n",
       " 'fiahnlly',\n",
       " 'fiailly',\n",
       " 'fiainlly',\n",
       " 'fiajlly',\n",
       " 'fiajnlly',\n",
       " 'fiaklly',\n",
       " 'fiaknlly',\n",
       " 'fiallly',\n",
       " 'fially',\n",
       " 'fialnlly',\n",
       " 'fialnly',\n",
       " 'fiamlly',\n",
       " 'fiamnlly',\n",
       " 'fianally',\n",
       " 'fianaly',\n",
       " 'fianblly',\n",
       " 'fianbly',\n",
       " 'fianclly',\n",
       " 'fiancly',\n",
       " 'fiandlly',\n",
       " 'fiandly',\n",
       " 'fianelly',\n",
       " 'fianely',\n",
       " 'fianflly',\n",
       " 'fianfly',\n",
       " 'fianglly',\n",
       " 'fiangly',\n",
       " 'fianhlly',\n",
       " 'fianhly',\n",
       " 'fianilly',\n",
       " 'fianily',\n",
       " 'fianjlly',\n",
       " 'fianjly',\n",
       " 'fianklly',\n",
       " 'fiankly',\n",
       " 'fianlaly',\n",
       " 'fianlay',\n",
       " 'fianlbly',\n",
       " 'fianlby',\n",
       " 'fianlcly',\n",
       " 'fianlcy',\n",
       " 'fianldly',\n",
       " 'fianldy',\n",
       " 'fianlely',\n",
       " 'fianley',\n",
       " 'fianlfly',\n",
       " 'fianlfy',\n",
       " 'fianlgly',\n",
       " 'fianlgy',\n",
       " 'fianlhly',\n",
       " 'fianlhy',\n",
       " 'fianlily',\n",
       " 'fianliy',\n",
       " 'fianljly',\n",
       " 'fianljy',\n",
       " 'fianlkly',\n",
       " 'fianlky',\n",
       " 'fianll',\n",
       " 'fianlla',\n",
       " 'fianllay',\n",
       " 'fianllb',\n",
       " 'fianllby',\n",
       " 'fianllc',\n",
       " 'fianllcy',\n",
       " 'fianlld',\n",
       " 'fianlldy',\n",
       " 'fianlle',\n",
       " 'fianlley',\n",
       " 'fianllf',\n",
       " 'fianllfy',\n",
       " 'fianllg',\n",
       " 'fianllgy',\n",
       " 'fianllh',\n",
       " 'fianllhy',\n",
       " 'fianlli',\n",
       " 'fianlliy',\n",
       " 'fianllj',\n",
       " 'fianlljy',\n",
       " 'fianllk',\n",
       " 'fianllky',\n",
       " 'fianlll',\n",
       " 'fianllly',\n",
       " 'fianllm',\n",
       " 'fianllmy',\n",
       " 'fianlln',\n",
       " 'fianllny',\n",
       " 'fianllo',\n",
       " 'fianlloy',\n",
       " 'fianllp',\n",
       " 'fianllpy',\n",
       " 'fianllq',\n",
       " 'fianllqy',\n",
       " 'fianllr',\n",
       " 'fianllry',\n",
       " 'fianlls',\n",
       " 'fianllsy',\n",
       " 'fianllt',\n",
       " 'fianllty',\n",
       " 'fianllu',\n",
       " 'fianlluy',\n",
       " 'fianllv',\n",
       " 'fianllvy',\n",
       " 'fianllw',\n",
       " 'fianllwy',\n",
       " 'fianllx',\n",
       " 'fianllxy',\n",
       " 'fianlly',\n",
       " 'fianllya',\n",
       " 'fianllyb',\n",
       " 'fianllyc',\n",
       " 'fianllyd',\n",
       " 'fianllye',\n",
       " 'fianllyf',\n",
       " 'fianllyg',\n",
       " 'fianllyh',\n",
       " 'fianllyi',\n",
       " 'fianllyj',\n",
       " 'fianllyk',\n",
       " 'fianllyl',\n",
       " 'fianllym',\n",
       " 'fianllyn',\n",
       " 'fianllyo',\n",
       " 'fianllyp',\n",
       " 'fianllyq',\n",
       " 'fianllyr',\n",
       " 'fianllys',\n",
       " 'fianllyt',\n",
       " 'fianllyu',\n",
       " 'fianllyv',\n",
       " 'fianllyw',\n",
       " 'fianllyx',\n",
       " 'fianllyy',\n",
       " 'fianllyz',\n",
       " 'fianllz',\n",
       " 'fianllzy',\n",
       " 'fianlmly',\n",
       " 'fianlmy',\n",
       " 'fianlnly',\n",
       " 'fianlny',\n",
       " 'fianloly',\n",
       " 'fianloy',\n",
       " 'fianlply',\n",
       " 'fianlpy',\n",
       " 'fianlqly',\n",
       " 'fianlqy',\n",
       " 'fianlrly',\n",
       " 'fianlry',\n",
       " 'fianlsly',\n",
       " 'fianlsy',\n",
       " 'fianltly',\n",
       " 'fianlty',\n",
       " 'fianluly',\n",
       " 'fianluy',\n",
       " 'fianlvly',\n",
       " 'fianlvy',\n",
       " 'fianlwly',\n",
       " 'fianlwy',\n",
       " 'fianlxly',\n",
       " 'fianlxy',\n",
       " 'fianly',\n",
       " 'fianlyl',\n",
       " 'fianlyly',\n",
       " 'fianlyy',\n",
       " 'fianlzly',\n",
       " 'fianlzy',\n",
       " 'fianmlly',\n",
       " 'fianmly',\n",
       " 'fiannlly',\n",
       " 'fiannly',\n",
       " 'fianolly',\n",
       " 'fianoly',\n",
       " 'fianplly',\n",
       " 'fianply',\n",
       " 'fianqlly',\n",
       " 'fianqly',\n",
       " 'fianrlly',\n",
       " 'fianrly',\n",
       " 'fianslly',\n",
       " 'fiansly',\n",
       " 'fiantlly',\n",
       " 'fiantly',\n",
       " 'fianully',\n",
       " 'fianuly',\n",
       " 'fianvlly',\n",
       " 'fianvly',\n",
       " 'fianwlly',\n",
       " 'fianwly',\n",
       " 'fianxlly',\n",
       " 'fianxly',\n",
       " 'fianylly',\n",
       " 'fianyly',\n",
       " 'fianzlly',\n",
       " 'fianzly',\n",
       " 'fiaolly',\n",
       " 'fiaonlly',\n",
       " 'fiaplly',\n",
       " 'fiapnlly',\n",
       " 'fiaqlly',\n",
       " 'fiaqnlly',\n",
       " 'fiarlly',\n",
       " 'fiarnlly',\n",
       " 'fiaslly',\n",
       " 'fiasnlly',\n",
       " 'fiatlly',\n",
       " 'fiatnlly',\n",
       " 'fiaully',\n",
       " 'fiaunlly',\n",
       " 'fiavlly',\n",
       " 'fiavnlly',\n",
       " 'fiawlly',\n",
       " 'fiawnlly',\n",
       " 'fiaxlly',\n",
       " 'fiaxnlly',\n",
       " 'fiaylly',\n",
       " 'fiaynlly',\n",
       " 'fiazlly',\n",
       " 'fiaznlly',\n",
       " 'fibanlly',\n",
       " 'fibnlly',\n",
       " 'ficanlly',\n",
       " 'ficnlly',\n",
       " 'fidanlly',\n",
       " 'fidnlly',\n",
       " 'fieanlly',\n",
       " 'fienlly',\n",
       " 'fifanlly',\n",
       " 'fifnlly',\n",
       " 'figanlly',\n",
       " 'fignlly',\n",
       " 'fihanlly',\n",
       " 'fihnlly',\n",
       " 'fiianlly',\n",
       " 'fiinlly',\n",
       " 'fijanlly',\n",
       " 'fijnlly',\n",
       " 'fikanlly',\n",
       " 'fiknlly',\n",
       " 'filanlly',\n",
       " 'filnlly',\n",
       " 'fimanlly',\n",
       " 'fimnlly',\n",
       " 'finally',\n",
       " 'finanlly',\n",
       " 'finlly',\n",
       " 'finnlly',\n",
       " 'fioanlly',\n",
       " 'fionlly',\n",
       " 'fipanlly',\n",
       " 'fipnlly',\n",
       " 'fiqanlly',\n",
       " 'fiqnlly',\n",
       " 'firanlly',\n",
       " 'firnlly',\n",
       " 'fisanlly',\n",
       " 'fisnlly',\n",
       " 'fitanlly',\n",
       " 'fitnlly',\n",
       " 'fiuanlly',\n",
       " 'fiunlly',\n",
       " 'fivanlly',\n",
       " 'fivnlly',\n",
       " 'fiwanlly',\n",
       " 'fiwnlly',\n",
       " 'fixanlly',\n",
       " 'fixnlly',\n",
       " 'fiyanlly',\n",
       " 'fiynlly',\n",
       " 'fizanlly',\n",
       " 'fiznlly',\n",
       " 'fjanlly',\n",
       " 'fjianlly',\n",
       " 'fkanlly',\n",
       " 'fkianlly',\n",
       " 'flanlly',\n",
       " 'flianlly',\n",
       " 'fmanlly',\n",
       " 'fmianlly',\n",
       " 'fnanlly',\n",
       " 'fnianlly',\n",
       " 'foanlly',\n",
       " 'foianlly',\n",
       " 'fpanlly',\n",
       " 'fpianlly',\n",
       " 'fqanlly',\n",
       " 'fqianlly',\n",
       " 'franlly',\n",
       " 'frianlly',\n",
       " 'fsanlly',\n",
       " 'fsianlly',\n",
       " 'ftanlly',\n",
       " 'ftianlly',\n",
       " 'fuanlly',\n",
       " 'fuianlly',\n",
       " 'fvanlly',\n",
       " 'fvianlly',\n",
       " 'fwanlly',\n",
       " 'fwianlly',\n",
       " 'fxanlly',\n",
       " 'fxianlly',\n",
       " 'fyanlly',\n",
       " 'fyianlly',\n",
       " 'fzanlly',\n",
       " 'fzianlly',\n",
       " 'gfianlly',\n",
       " 'gianlly',\n",
       " 'hfianlly',\n",
       " 'hianlly',\n",
       " 'ianlly',\n",
       " 'ifanlly',\n",
       " 'ifianlly',\n",
       " 'iianlly',\n",
       " 'jfianlly',\n",
       " 'jianlly',\n",
       " 'kfianlly',\n",
       " 'kianlly',\n",
       " 'lfianlly',\n",
       " 'lianlly',\n",
       " 'mfianlly',\n",
       " 'mianlly',\n",
       " 'nfianlly',\n",
       " 'nianlly',\n",
       " 'ofianlly',\n",
       " 'oianlly',\n",
       " 'pfianlly',\n",
       " 'pianlly',\n",
       " 'qfianlly',\n",
       " 'qianlly',\n",
       " 'rfianlly',\n",
       " 'rianlly',\n",
       " 'sfianlly',\n",
       " 'sianlly',\n",
       " 'tfianlly',\n",
       " 'tianlly',\n",
       " 'ufianlly',\n",
       " 'uianlly',\n",
       " 'vfianlly',\n",
       " 'vianlly',\n",
       " 'wfianlly',\n",
       " 'wianlly',\n",
       " 'xfianlly',\n",
       " 'xianlly',\n",
       " 'yfianlly',\n",
       " 'yianlly',\n",
       " 'zfianlly',\n",
       " 'zianlly'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits1(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finally'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits1(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faintly', 'finally', 'finely', 'frankly'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits2(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对编辑距离更小的单词赋予更高的优先级\n",
    "candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finally'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果候选词中存在两个单词的编辑距离相同,则可以通过使用max(candidates,key=WORD_COUNTS.get)函数从词汇字典WORDS_COUNTS中选取出现频率最高的词来作为有效词."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    '''\n",
    "    Get the best correct spelling for the input word\n",
    "    '''\n",
    "    candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "    return max(candidates, key=WORDS_COUNTS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FIANLLY'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('FIANLLY') #函数对大小写敏感,无法校正非小写的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对上面的函数进行改进,使其能够同时校正大写和小写的单词.\n",
    "def correct_match(match):\n",
    "    '''\n",
    "    该函数的逻辑是存储单词的原始大小写格式,\n",
    "    然后将所有字母转换为大小写字母,更正拼写错误,\n",
    "    最后使用case_of函数将其重新转换回初始的大小写格式.\n",
    "    '''\n",
    "    word = match.group(0)\n",
    "    def case_of(text):\n",
    "        return (str.upper if text.isupper() else\n",
    "               str.lower if text.islower() else\n",
    "               str.title if text.istitle() else\n",
    "               str)\n",
    "    \n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def correct_text_generic(text):\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    }
   ],
   "source": [
    "print correct_text_generic('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINALLY\n"
     ]
    }
   ],
   "source": [
    "print correct_text_generic('FIANLLY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上采用的方法存在不足之处,如果单词没有出现在词汇字典中,就有可能无法被校正."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('finally', 1.0)]\n",
      "[('flat', 0.85), ('float', 0.15)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "print suggest('fianlly')\n",
    "\n",
    "print suggest('flaot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有PyEnchant库,它基于enchant库,以及aspell-python库,它是目前很流行的GNU Aspell的一个python 封装."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取\n",
    "**词素**是任何自然语言中最小的独立单元.由词干和词缀(affixe)组成.\n",
    "\n",
    "**词缀**是指前缀,后缀等词语单元,它们附加到词干熵以改变其含义或创建一个新单词.\n",
    "\n",
    "**词干**也经常称为单词的基本形式,可以通过在词干上添加词缀来创建新词,这个过程称为\"词形变化\".\n",
    "\n",
    "相反的过程是从单词的变形形式中获得单词的基本形式,这称为**词干提取**.\n",
    "\n",
    "对于词干提取器,nltk包有几种实现算法.这些词干提取器包含在stem模块中,该模块继承了nltk.stem.api模块中的StemmerI接口.\n",
    "\n",
    "最受欢迎的词干提取器之一是波特词干提取器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n"
     ]
    }
   ],
   "source": [
    "print ps.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strang\n"
     ]
    }
   ],
   "source": [
    "print ps.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**兰卡斯特词干提取器Lancaster stemmer基于兰卡斯特词干算法,通常也称为佩斯/哈斯科词干提取器(Paice/Husk stemmer).**\n",
    "\n",
    "**该词干提取器是一个迭代提取器,具有超过120条规则来具体说明如何删减或替换词缀以获得词干.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "print ls.stem('lying')\n",
    "print ls.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RegexpStemmer词干提取器,RegexStemmer它可以根据用户定义的规则建立自己的词干提取器**\n",
    "\n",
    "**它使用正则表达式来识别词语中的形态学词缀,并且删除与之匹配的任何部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n",
      "ly\n",
      "strange\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "\n",
    "print rs.stem('jumping'),rs.stem('jumps'), rs.stem('jumped')\n",
    "\n",
    "print rs.stem('lying')\n",
    "\n",
    "print rs.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用SnowballStemmer来对其他语言的单词进行词干提取.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: (u'danish', u'dutch', u'english', u'finnish', u'french', u'german', u'hungarian', u'italian', u'norwegian', u'porter', u'portuguese', u'romanian', u'russian', u'spanish', u'swedish')\n",
      "autobahn\n",
      "spring\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "#德语\n",
    "ss = SnowballStemmer('german')\n",
    "print 'Supported Languages:', SnowballStemmer.languages\n",
    "print ss.stem('autobahnen')\n",
    "print ss.stem('springen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "波特词干提取器是目前最常用的词干提取器,但是在实际执行词干提取时,你还是应该根据具体问题来选择词干提取器,并经过反复试验以验证提取器效果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词形还原\n",
    "词形还原的过程与词干提取非常相似,去除词缀以获得单词的基本形式.这种基本形式称为**根词**,而不是**词干**.\n",
    "\n",
    "根词也称为**词元**,始终存在于词典中.\n",
    "\n",
    "词形还原的过程比词干提取慢得多,因为它涉及一个附加步骤,**当且仅当该词元存在于词典中**时,才通过去除词缀形成根形式或词元.\n",
    "\n",
    "nltk包有一个强大的词形还原模块,它使用WordNet,单词的句法和语义来获得根词或词元.\n",
    "\n",
    "词性主要包含三个实体--名词,动词和形容词----最常见于自然语言."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "print wnl.lemmatize('cars','n')\n",
    "print wnl.lemmatize('men','n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print wnl.lemmatize('running','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "print wnl.lemmatize('ate','v')\n",
    "print wnl.lemmatize('saddest','a')\n",
    "print wnl.lemmatize('fancier','a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNetLemmatizer使用单词及其词性,通过比对WordNet语料库,并采用递归技术删除词缀直到在词汇网络中找到匹配项,\n",
    "\n",
    "最终获得输入词的基本形式或词元.如果没有找到匹配项,则将返回输入词(输入词不做任何变化)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "#如果词性错误,那么词性还原就会失效\n",
    "print wnl.lemmatize('ate','n')\n",
    "print wnl.lemmatize('fancier','v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理解文本句法和结构\n",
    "本节将关注以下技术:\n",
    "- 词性(POS)标签\n",
    "- 浅层分析\n",
    "- 基于依存关系的解析\n",
    "- 基于成分结构的解析\n",
    "\n",
    " 1. 下载spacy的英文语言模型\n",
    " 2. 斯坦福分析器,有斯坦福大学开发的基于java的语言分析器,它能够帮助我们解析句子以了解其底层结果.我们将使用斯坦福分析器和nltk来执行基于依存关系的解析以及基于成分结构的解析.\n",
    " \n",
    "### 词性标注\n",
    "词性是基于语法语境和词语作用的具体词汇分类.对单词进行分类并标记POS标签称为词性标注或POS标注.\n",
    "\n",
    "更多信息请参阅www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/Pann-Treebank-Tagset.pdf 中找到有关各种pos标签机器标注的更多信息.\n",
    "\n",
    "#### POS标签器推荐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', u'DET'), ('brown', u'ADJ'), ('fox', u'NOUN'), ('is', u'VERB'), ('quick', u'ADJ'), ('and', u'CONJ'), ('he', u'PRON'), ('is', u'VERB'), ('jumping', u'VERB'), ('over', u'ADP'), ('the', u'DET'), ('lazy', u'ADJ'), ('dog', u'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "#第一种方法是使用nltk推荐pos_tag()函数,基于Penn Treebank\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "#第二种方法是使用pattern模块通过以下代码段获取句子的POS标签\n",
    "from pattern.en import tag\n",
    "tagged_sent = tag(sentence)\n",
    "print tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "#第三种方法是构建自己的POS标签器\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "\n",
    "print train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**评估标签器的性能**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145415819537\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "\n",
    "print dt.evaluate(test_data)\n",
    "print dt.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用正则表达式和RegexpTagger来尝试构建一个性能更好的标签器\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$','VBG'),  #gerunds\n",
    "    (r'.*ed$','VBD'),   #simple past\n",
    "    (r'.*es$', 'VBZ'),  #3rd singular present\n",
    "    (r'.*ould$','MD'),  #modals\n",
    "    (r'.*\\'s$','NN$'),  #possessive nouns\n",
    "    (r'.*s$','NNS'),    #plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$','CD'), #cardinal numbers\n",
    "    (r'.*', 'NN')       # nouns (default) ...\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240391131765\n"
     ]
    }
   ],
   "source": [
    "rt = RegexpTagger(patterns)\n",
    "print rt.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print rt.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n元分词是来自文本序列或语音序列的n个连续项.这些项可以由单词,音素,字母,字符或音节组成."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861361215994\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', None), ('dog', None)]\n",
      "0.134669377481\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n",
      "0.0806467228192\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "#testing performance of unigram tagger\n",
    "print ut.evaluate(test_data)\n",
    "print ut.tag(tokens)\n",
    "\n",
    "#testing performance of bigram tagger\n",
    "print bt.evaluate(test_data)\n",
    "print bt.tag(tokens)\n",
    "\n",
    "#testing performance of trigram tagger\n",
    "print tt.evaluate(test_data)\n",
    "print tt.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910155871817\n",
      "[('The', u'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', 'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#通过创建一个包含标签列表的组合标签器以及使用backoff标签器,我们将尝试组合运用所有的标签器(串联所有的标签器)\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, taggers=[UnigramTagger, BigramTagger, TrigramTagger], backoff=rt)\n",
    "print ct.evaluate(test_data)\n",
    "print ct.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930680607997\n",
      "[('The', u'DT'), ('brown', u'JJ'), ('fox', u'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'JJ'), ('dog', u'VBG')]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "对于最终的标签器,我们将使用有监督的分类算法来训练它.\n",
    "ClassifierBasedPOSTagger类使我们能够使用Classifier_builder参数中的有监督机器学习算法来训练标签器.该函数用于从训练数据中生成各种特征.\n",
    "在这里,我们使用的分类器是NaiveBayesClassifier,它使用贝叶斯定理构建概率分类器,假设特征之间是独立的.\n",
    "'''\n",
    "#如何基于分类方法构建POS标签器并对其进行评估\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data, classifier_builder=NaiveBayesClassifier.train)\n",
    "print nbt.evaluate(test_data)\n",
    "print nbt.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅层分析\n",
    "浅层分析(shallow parsing)也称为浅分析(light parsing)或组块分析(chunking).是分析句子结构的一种技术,它将句子分解成最小的组成部分,然后将它们组合成更高级的短语.\n",
    "\n",
    "在浅层分析中,主要的关注焦点是识别这些短语或语块.它的主要目的是获得语义上有意义的短语,并观察它们之间的关系.\n",
    "\n",
    "### 浅层分析器推荐\n",
    "在这里,我们将使用pattern包创建一个浅层分析器,用以从句子中提取有意义的语块."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence('The/DT/B-NP/O brown/JJ/I-NP/O fox/NN/I-NP/O is/VBZ/B-VP/O quick/JJ/B-ADJP/O and/CC/O/O he/PRP/B-NP/O is/VBZ/B-VP/O jumping/VBG/I-VP/O over/IN/B-PP/B-PNP the/DT/B-NP/I-PNP lazy/JJ/I-NP/I-PNP dog/NN/I-NP/I-PNP')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "from pattern.en import parsetree\n",
    "tree = parsetree(sentence)\n",
    "\n",
    "print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    print sentence_tree.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP -> [(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN')]\n",
      "VP -> [(u'is', u'VBZ')]\n",
      "ADJP -> [(u'quick', u'JJ')]\n",
      "NP -> [(u'he', u'PRP')]\n",
      "VP -> [(u'is', u'VBZ'), (u'jumping', u'VBG')]\n",
      "PP -> [(u'over', u'IN')]\n",
      "NP -> [(u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print chunk.type, '->', [(word.string, word.type) for word in chunk.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前缀I,O和B,即分块技术领域里十分流行的IOB标注,I,O和B分别表示内部,外部和开头.\n",
    "\n",
    "标签前面的B-前缀表示它是块的开始,而I-前缀则表示它在块内.O标签表示标识不属于任何块.\n",
    "\n",
    "当后续标签跟当前语块的标签类型相同,并且它们之间不存在O标签时,则对当前块使用B-标签."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    print sentence_tree.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP -> [(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN')]\n",
      "VP -> [(u'is', u'VBZ')]\n",
      "ADJP -> [(u'quick', u'JJ')]\n",
      "NP -> [(u'he', u'PRP')]\n",
      "VP -> [(u'is', u'VBZ'), (u'jumping', u'VBG')]\n",
      "PP -> [(u'over', u'IN')]\n",
      "NP -> [(u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print chunk.type, '->', [(word.string, word.type) for word in chunk.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,构建一些通用函数,更好的解析和可视化浅层分析的语句树,还可以在分析常见句子时重复使用它们."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parsetree,Chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def create_sentence_tree(sentence, lemmatize=False):\n",
    "    sentence_tree = parsetree(sentence,\n",
    "                             relations=True,\n",
    "                             lemmata=lemmatize) # if you want to lemmatize the tokens\n",
    "    return sentence_tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get various constituents of the parse tree\n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "\n",
    "#process the shallow parsed tree into an easy to understand format\n",
    "def process_sentence_tree(sentence_tree):\n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "        (item.type,\n",
    "        [\n",
    "            (w.string,w.type)\n",
    "            for w in item.words\n",
    "        ]\n",
    "        )\n",
    "        if type(item) == Chunk\n",
    "        else ('-',\n",
    "             [\n",
    "                 (item.string, item.type)\n",
    "             ]\n",
    "             )\n",
    "             for item in tree_constituents\n",
    "    ]\n",
    "    return processed_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print the sentence tree using nltk's Tree syntax\n",
    "def print_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0],\n",
    "            [\n",
    "                Tree(x[1],[x[0]])\n",
    "                for x in item[1]\n",
    "            ]\n",
    "            )\n",
    "        for item in processed_tree\n",
    "    ]\n",
    "    \n",
    "    tree = Tree('5', processed_tree)\n",
    "    print tree\n",
    "    \n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0],\n",
    "            [\n",
    "                Tree(x[1],[x[0]])\n",
    "                for x in item[1]\n",
    "            ]\n",
    "            )\n",
    "        for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S', processed_tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence('The/DT/B-NP/O/NP-SBJ-1 brown/JJ/I-NP/O/NP-SBJ-1 fox/NN/I-NP/O/NP-SBJ-1 is/VBZ/B-VP/O/VP-1 quick/JJ/B-ADJP/O/O and/CC/O/O/O he/PRP/B-NP/O/NP-SBJ-2 is/VBZ/B-VP/O/VP-2 jumping/VBG/I-VP/O/VP-2 over/IN/B-PP/B-PNP/O the/DT/B-NP/I-PNP/O lazy/JJ/I-NP/I-PNP/O dog/NN/I-NP/I-PNP/O')\n",
      "\n",
      "[(u'NP', [(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN')]), (u'VP', [(u'is', u'VBZ')]), (u'ADJP', [(u'quick', u'JJ')]), ('-', [(u'and', u'CC')]), (u'NP', [(u'he', u'PRP')]), (u'VP', [(u'is', u'VBZ'), (u'jumping', u'VBG')]), (u'PP', [(u'over', u'IN')]), (u'NP', [(u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')])]\n"
     ]
    }
   ],
   "source": [
    "t = create_sentence_tree(sentence)\n",
    "print t\n",
    "print \n",
    "pt = process_sentence_tree(t)\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (NP (DT the) (JJ lazy) (NN dog)))\n"
     ]
    }
   ],
   "source": [
    "print_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建自己的浅层分析器\n",
    "我们将使用正则表达式,基于标签的学习器等技术构建自己的浅层分析器.\n",
    "\n",
    "在nltk中,可以使用**treebank语料库**,它带有**语块标注**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]\n",
    "\n",
    "print train_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', u'DT'), (u'quick', u'JJ'), (u'fox', u'NN'), (u'jumped', u'VBD'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n",
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "我们的数据点是使用短语和POS标签完成标注的句子,这将有助于训练浅层分析器.\n",
    "使用正则表达式进行浅层分析,还会使用分块和加缝隙的概念.\n",
    "通过分块,我们可以完成并指定特定的模式来识别想要在句子中分块或分段的内容.\n",
    "加缝隙则与分块过程相反,在该过程中,我们指定一些特定的标识使其不属于任何语块,然后形成除这些标识以外的必要语块.\n",
    "'''\n",
    "\n",
    "sample_sentence = 'the quick fox jumped over the lazy dog'\n",
    "\n",
    "from nltk.chunk import RegexpParser\n",
    "from pattern.en import tag\n",
    "\n",
    "tagged_simple_sent = tag(sample_sentence)\n",
    "print tagged_simple_sent\n",
    "\n",
    "chunk_grammar = '''NP:{<DT>?<JJ>*<NN.*>}'''\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP\n",
      "    the/DT\n",
      "    quick/JJ\n",
      "    fox/NN\n",
      "    jumped/VBD\n",
      "    over/IN\n",
      "    the/DT\n",
      "    lazy/JJ\n",
      "    dog/NN))\n"
     ]
    }
   ],
   "source": [
    "#illustrate NP chunking based on explicit chink patterns\n",
    "chink_grammar = '''NP: {<.*>+} # chunk everything as NP }<VBD|IN>+{'''\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请记住,语块是包含在组块(语块)集合中的标识序列,缝隙则是被排除在语块之外的标识或标识序列\n",
    "\n",
    "我们要训练一个更为通用的基于正则表达式的浅层分析器,并在我们的测试treebank数据上检测其性能.\n",
    "\n",
    "训练一个更为通用的基于正则表达式的浅层分析器,并在我们的测试treebank数据上检测其性能.在程序内部,需要执行几个步骤来完成此分析器.\n",
    "\n",
    "- 首选,需要将nltk中用于表示被解析语句的Tree结构转换为ChunkString对象\n",
    "- 然后,使用定义好的分块和加缝隙规则创建一个RegexpParser对象.\n",
    "- 最后,使用ChunkRule和ChinkRule类及其对象创建完整的,带有必要语块的浅层分析树."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n",
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP is/VBZ jumping/VBG)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  54.5%%\n",
      "    Precision:     25.0%%\n",
      "    Recall:        52.5%%\n",
      "    F-Measure:     33.9%%\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = tag(sentence)\n",
    "print tagged_sentence\n",
    "\n",
    "grammar = '''\n",
    "NP: {<DT>?<JJ>?<NN.*>}\n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}\n",
    "VP: {<MD>?<VB.*>+}\n",
    "'''\n",
    "\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sentence)\n",
    "print c\n",
    "print rc.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,我们将使用分好块并标记好的treebank训练数据,构建一个浅层分析器.\n",
    "\n",
    "将用到两个分块函数:\n",
    "- 一个是tree2conlltags函数,它可以为每个词元获取三组数据--单词,标签和块标签;\n",
    "- 另一个是colltags2tree函数,它可以从上述三元组数据中生成分析树.\n",
    "\n",
    "请记住,块标签使用前面提到的IOB格式:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.util import tree2conlltags,conlltags2tree\n",
    "\n",
    "train_sent = train_data[7]\n",
    "print train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', u'DT', u'B-NP'),\n",
       " (u'Lorillard', u'NNP', u'I-NP'),\n",
       " (u'spokewoman', u'NN', u'I-NP'),\n",
       " (u'said', u'VBD', u'O'),\n",
       " (u',', u',', u'O'),\n",
       " (u'``', u'``', u'O'),\n",
       " (u'This', u'DT', u'B-NP'),\n",
       " (u'is', u'VBZ', u'O'),\n",
       " (u'an', u'DT', u'B-NP'),\n",
       " (u'old', u'JJ', u'I-NP'),\n",
       " (u'story', u'NN', u'I-NP'),\n",
       " (u'.', u'.', u'O')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc = tree2conlltags(train_sent)\n",
    "wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree = conlltags2tree(wtc)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来,我们定义了一个函数conll_tag_chunks()从分块标注好的句子中提取POS和块标签. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t,c) for (w,t,c) in sent] for sent in tagged_sents]\n",
    "\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个NGramTagChunker类,将标记好的句子作为训练输入,获取它们的WTC三元组,即单词(word),POS标签(POS tag)和块标签(Chunk tag)三元组\n",
    "\n",
    "并使用UnigramTagger作为backoff标签器训练一个BigramTagger.我们还将定义一个parse()函数来对新的句子执行浅层分析."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sentences, tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "        '''\n",
    "        基于语句WTC三元组的n元分词标签训练浅层分析器.\n",
    "        将一列训练语句作为输入,并使用分好块的分析树元数据做标注.\n",
    "        使用conll_tag_chunks()函数来获取所有分块分析树的WTC三元组数据列表\n",
    "        然后,使用这些三元组数据列表训练出一个bigram标签器,它使用Unigram标签器作为backoff标签器,\n",
    "        并且将训练模型存储在self.chunk_tagger中.\n",
    "        \n",
    "        这里使用tagger_classes参数来分析其他n元分词标签.\n",
    "        '''\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "        \n",
    "    def parse(self, tagged_sentence):\n",
    "        '''\n",
    "        评估测试数据上的标签并对新的句子进行浅层分析.\n",
    "        该函数使用经POS标注的句子作为输入,从句子中分离出POS标签,\n",
    "        并使用我们训练完的self.chunk_tagger获取句子的IOB块标签.\n",
    "        然后,将其与原始句子标识相结合,并使用conlltags2tree()函数获取最终的浅层分析树\n",
    "        '''\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        pos_tags = [tag for word, tag in tagged_sentence]\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags = [(word, pos_tag,chunk_tag) for ((word, pos_tag), chunk_tag) in zip(tagged_sentence, chunk_tags)]\n",
    "        \n",
    "        return conlltags2tree(wpc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.6%%\n",
      "    Precision:     98.4%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:     99.2%%\n",
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  is/VBZ\n",
      "  jumping/VBG\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "#train the shallow parser\n",
    "ntc = NGramTagChunker(train_data)\n",
    "\n",
    "#test parser performance on test data\n",
    "print ntc.evaluate(test_data)\n",
    "\n",
    "#parse our sample sentence\n",
    "tree = ntc.parse(tagged_sentence)\n",
    "print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在在conll2000语料库熵对我们的分析器进行训练和评估.\n",
    "\n",
    "conll2000语料库是一个更大的语料库,它包含了'华尔街日报'摘录.\n",
    "\n",
    "将前7500个句子上训练我们的分析器,并在其余3448个句子上进行性能测试."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print train_wsj_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        85.9%%\n",
      "    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "#train the shallow parser\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "\n",
    "#test performance on the test data\n",
    "print tc.evaluate(test_wsj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
