{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现数值格式的特征输入,你需要清洗,规范化和预处理初始文本数据.通常,文本语料库和原始文本的数据格式既非准确的,也非规范的.文本处理,涉及使用各种技术将原始文本转换成定义良好的语言成分序列,这些序列具有标准的结构和标记.\n",
    "\n",
    "以下是将在本章中探讨的一些主流文本预处理技术:\n",
    "- 切分(tokenization)\n",
    "- 标注(tagging)\n",
    "- 分块(chunking)\n",
    "- 词干提取(stemming)\n",
    "- 词性还原(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本切分\n",
    "标识(token)是具有一定的句法语义且独立的最小文本成分.\n",
    "\n",
    "一段文本或一个文本文件具有几个组成部分,包括可以进一步细分为从句,短语和单词的语句.\n",
    "\n",
    "最流行的文本切分技术包括句子切分和词语切分,用于将文本语料库分解成句子,并将每个句子分解成单词.\n",
    "\n",
    "文本切分可以定义为将文本数据分解或拆分为具有更小且有意义的成分的过程.\n",
    "\n",
    "### 句子切分\n",
    "句子切分(sentence tokenization)是将文本语料库分解成句子的过程.\n",
    "\n",
    "执行句子切分有多种技术,基本技术包括在句子之间寻找特定的分隔符.\n",
    "\n",
    "我们将使用NLTK框架进行切分,该框架提供用于执行句子切分的各种接口.我们将主要关注一下句子切分器:\n",
    "- sent_tokenize\n",
    "- PunkSentenceTokenizer\n",
    "- RegexpTokenizer\n",
    "- 预先训练的句子切分模型\n",
    "\n",
    "将文本分割成句子之前,需要一些测试该操作的文本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = '''we will discuss briefly about the basic syntax, structure and design philosophies, \n",
    "                There is a defined hierarchical syntax for Python code which you should remember when writing code!\n",
    "                Python is a really powerful programming language!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n"
     ]
    }
   ],
   "source": [
    "print len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "print alice[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.sent_tokenize函数是nltk推荐的模型的句子切分函数.它内部使用一个PunktSentenceTokenizer类的实例."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 2\n",
      "Sample text sentences :-\n",
      "['we will discuss briefly about the basic syntax, structure and design philosophies, \\n                There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[u\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " u\"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\",\n",
      " u'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.',\n",
      " u\"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " u'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "#以下代码段展示了该函数在实例文本中的基本用法:\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print 'Total sentences in sample_text:', len(sample_sentences)\n",
    "print 'Sample text sentences :-'\n",
    "pprint(sample_sentences)\n",
    "print '\\nTotal sentences in alice:', len(alice_sentences)\n",
    "print 'First 5 sentences in alice:-'\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对德语文本进行句子切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import europarl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "#文本长度\n",
    "print len(german_text)\n",
    "#文本的前100个字符\n",
    "print german_text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "#使用默认的sent_tokenize切分器\n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "#从nltk源加载的预训练的德语切分器\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "#德语切分器也是属于PunktSentenceTokenizer类型\n",
    "print type(german_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的语句输出可以看出german_tokenizer是PunktSentenceTokenizer的一个实例,它专门用来处理德语.\n",
    "\n",
    "接下来,对比使用默认切分器和使用德语切分器的效果有什么不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "print german_sentences_def == german_sentences\n",
    "for sent in german_sentences[0:5]:\n",
    "    print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we will discuss briefly about the basic syntax, structure and design philosophies, \\n                There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "#使用默认的PunktSentenceTokenizer类也能很方便的实现句子切分.\n",
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用正则表达式的模式来切分句子:RegexpTokenizer类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we will discuss briefly about the basic syntax, structure and design philosophies, \\n                There is a defined hierarchical syntax for Python code which you should remember when writing code!',\n",
      " '                Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词语切分\n",
    "词语切分(word tokenization)是将句子分解或分割成其组成单词的过程.\n",
    "\n",
    "句子是单词的集合,通过词语切分,在本质上即是将一个句子分割成单词列表,该单词列表又可以重建句子.\n",
    "\n",
    "词语切分在很多过程中都是非常重要的,特别是在文本清洗和规范化时,诸如词干提取和词形还原这类基于词干,标识信息的操作会子啊每个单词上实施.\n",
    "\n",
    "nltk的主流词语切分接口:\n",
    "- word_tokenize\n",
    "- TreebankWordTokenizer\n",
    "- RegexpTokenizer\n",
    "- 从RegexpTokenizer继承的切分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.word_tokenize函数是nltk默认并推荐的词语切分器.该切分器实际上是TreebankWordTokenizer类的一个实例或对象,并且是该核心类的一个封装.\n",
    "\n",
    "TreebankWordTokenizer基于Penn Treebank,并使用各种正则表达式来分割文本.该切分器的一些主要功能包括:\n",
    "\n",
    "- 分割和分离出现在句子末尾的句点\n",
    "- 分割和分离空格前的逗号和单引号\n",
    "- 将大多数标点符号分割成独立标识\n",
    "- 分割常规的缩写词--例如将'don't'分割成'do'和'n't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下代码使用正则表达式执行词语切**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+' #定义正则表达式\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+' #定义正则表达式\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print word_indices\n",
    "print [sentence[start:end] for start, end in word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "#使用r'\\w+|[^\\w\\s]+'模式将句子切分成独立的字母和非字母标识\n",
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "#whitespacetokenizer基于诸如缩进符,换行符及空格的空白字符将句子分割成单词\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本规范化\n",
    "- 文本清洗\n",
    "- 大小写转换\n",
    "- 词语校正\n",
    "- 停用词删除\n",
    "- 词干提取\n",
    "- 词形还原\n",
    "\n",
    "文本规范化通常也称为文本清洗或转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "corpus = [\"the brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "         \"@@You'll (learn) a **lot** in the book.Python is  an amazing language!@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本清洗\n",
    "对于清洗html之类的数据源中提取有意义的文本,可以使用nltk的clean_html()函数,或者BeautifulSoup库来解析HTML数据,或者正则表达式,xpath和lxml库来解析XML数据."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本切分\n",
    "文本切分和删除多余字符的顺序取决于你要解决的问题和你正在处理的数据."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    '''\n",
    "    接收文本数据,从中提取句子,最后将每个句子划分成标识.\n",
    "    这些标识可以是单词,特殊字符或标点符号.\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '**lot**',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book.Python',\n",
      "   'is',\n",
      "   'an',\n",
      "   'amazing',\n",
      "   'language',\n",
      "   '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除特殊字符\n",
    "文本规范化中的一个重要任务是删除多余和特殊的字符,诸如特殊符号或标点符号.\n",
    "\n",
    "删除特殊字符的原因是分析文本并提取基于NLP和机器学习的特征或信息时,标点符号或特殊字符往往没有多大的意义."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    '''\n",
    "    string.punctuation由所有可能的特殊字符/符号组成,并从中创建一个正则表达式模式.\n",
    "    使用正则表达式sub算法删除特殊字符\n",
    "    filter函数删除空标识\n",
    "    '''\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) #哇,还可以这么写\n",
    "    filtered_tokens = filter(None, [pattern.sub('',token) for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list_1 = [filter(None, [remove_characters_after_tokenization(tokens) for tokens in sentence_tokens]) \n",
    "                   for sentence_tokens in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race']], [['Hey', 'that', 's', 'a', 'great', 'deal'], ['I', 'just', 'bought', 'a', 'phone', 'for', '199']], [['You', 'll', 'learn', 'a', 'lot', 'in', 'the', 'bookPython', 'is', 'an', 'amazing', 'language']]]\n"
     ]
    }
   ],
   "source": [
    "print filtered_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    '''\n",
    "    在文本切分之前删除特殊字符\n",
    "    '''\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        #保留撇号和句号\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN =  r'[^a-zA-Z0-9 ]'\n",
    "        filtered_sentence = re.sub(PATTERN,r'',sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the bookPython is  an amazing language']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print filtered_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"the brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book.Python is  an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
    "print cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展缩写词\n",
    "缩写词是词或音节的缩短形式."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
