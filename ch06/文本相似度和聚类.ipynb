{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 文本相似度\n",
    "\n",
    "文本数据是非结构化的和高噪声的.在执行文本分类时,拥有标记合理的训练数据和有监督学习大有裨益.\n",
    "\n",
    "但是,文档聚类是一个无监督的学习过程,我们将尝试通过让机器学习各种各样的文本文档及其特征,相似\n",
    "\n",
    "度以及它们之间的差异,来将文档分割和分类为单独的类别.\n",
    "\n",
    "本章将重点介绍与文本相似度,距离度量和无监督ML算法相关的几个概念,以回答以下问题:\n",
    "\n",
    "- 如何衡量文档之间的相似度?\n",
    "- 如何使用距离测量值称为度量?\n",
    "- 什么时候距离测量值称为度量?\n",
    "- 如何聚类或组合类似的文档?\n",
    "- 可以可视化文档聚类吗?\n",
    "\n",
    "### 相似度测量\n",
    "相似度或距离测量值通常是用来衡量两个实体之间的接近程度的,其中实体可以是任何文本形式,例如文档,句子甚至是短语.\n",
    "\n",
    "实体之间的相似程度由两个主要因素决定:\n",
    "- 实体的固有属性或特征\n",
    "- 测量公式及其特性\n",
    "\n",
    "### 无监督的机器学习算法\n",
    "无监督的机器学习算法属于ML算法系列,从数据的各种属性和特征中发现其中潜在的,隐藏的结构和模式.\n",
    "\n",
    "## 文本规范化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新停用词列表\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr','mrs','come','go','get','tell','listen','one','two','three','four',\n",
    "                                'five','six','seven','eight','nine','zero','join','find','make','say','ask',\n",
    "                                'tell','see','try','back','also']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用正则表达式从文本主体中提取文本标识\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, lemmatize=True,\n",
    "                    only_text_chars=False,\n",
    "                    tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text) #html分析\n",
    "        text = expand_contractions(text, CONTRACTION_MAP) #扩展缩写词\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text) #词形还原\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # 删除停用词及特殊字符\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        \n",
    "        #提取文本标识\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "            \n",
    "        if tokenize:\n",
    "            text = tokenize_text(text) #分词\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1,1), min_df=0.0, max_df=1.0):\n",
    "    '''\n",
    "    min_df:忽略文档频率低于该阈值的特征\n",
    "    max_df:忽略文档频率高于该阈值的特征\n",
    "    '''\n",
    "    feature_type = feature_type.lower().strip()\n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise Exception('Wrong feature type entered.Possible values: \"binary\",\"frequency\",\"tfidf\"')\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本相似度\n",
    "文本相似度分析的主要目的是分析和测量两个文本实体彼此距离的远近.\n",
    "\n",
    "文本相似度分析的目的大致分为以下两个方面:\n",
    "- 词汇相似度:通过句法,结构和内容研究文本文档的内容,并根据这些参数测量其相似度\n",
    "- 语义相似度:首先找出文档的语义,含义和上下文,然后找出它们彼此的距离.在这方面,依存语法和实体识别是很有用的工具\n",
    "\n",
    "接下来,将主要介绍以下两个领域的文本相似度:\n",
    "- 词项相似度: 在这里,将测量每个标识或单词之间的相似度\n",
    "- 文档相似度: 在这里,将测量整个文本文档之间的相似度\n",
    "\n",
    "## 词项相似度分析\n",
    "从分析单独的单词标识相似度入手.我们可以使用词项相似度分析中的部分技术来纠正拼写错误的词项.进行词项相似度分析,\n",
    "\n",
    "首先需要选择一种单词标识方法,然后指定一种距离度量.我们将使用如下单词表示方法:\n",
    "\n",
    "- 字符向量化\n",
    "- 字符袋(Bag of Character)向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_terms(terms):\n",
    "    terms = [term.lower() for term in terms]\n",
    "    terms = [np.array(list(term)) for term in terms]\n",
    "    terms = [np.array([ord(char) for char in term]) for term in terms]\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "\n",
    "def boc_term_vectors(word_list):\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "    unique_chars = np.unique(np.hstack([list(word) for word in word_list]))\n",
    "    word_list_term_counts = [{char: count for char, count in itemfreq(list(word))} for word in word_list]\n",
    "    boc_vectors = [np.array([int(word_term_counts.get(char, 0)) for char in unique_chars])\n",
    "                  for word_term_counts in word_list_term_counts]\n",
    "    \n",
    "    return list(unique_chars), boc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root:[ 98 101 108 105 101 118 101]\n",
      "term1:[ 98 101 108 101 105 118 101]\n",
      "term2:[ 98  97 114 103  97 105 110]\n",
      "term3:[101 108 101 112 104  97 110 116]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = 'Believe'\n",
    "term1 = 'beleive'\n",
    "term2 = 'bargain'\n",
    "term3 = 'Elephant'\n",
    "terms = [root, term1, term2, term3]\n",
    "#Character vectorization\n",
    "vec_root, vec_term1, vec_term2, vec_term3 = vectorize_terms(terms)\n",
    "#show vector representations\n",
    "print '''\n",
    "root:{}\n",
    "term1:{}\n",
    "term2:{}\n",
    "term3:{}\n",
    "'''.format(vec_root, vec_term1, vec_term2, vec_term3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['a', 'b', 'e', 'g', 'h', 'i', 'l', 'n', 'p', 'r', 't', 'v']\n",
      "\n",
      "root:[0 1 3 0 0 1 1 0 0 0 0 1]\n",
      "term1:[0 1 3 0 0 1 1 0 0 0 0 1]\n",
      "term2:[2 1 0 1 0 1 0 1 0 1 0 0]\n",
      "term3:[1 0 2 0 1 0 1 1 1 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bag of characters vectorization\n",
    "features,(boc_root, boc_term1, boc_term2, boc_term3) = boc_term_vectors(terms)\n",
    "#show features and vector representations\n",
    "print 'Features:', features\n",
    "print '''\n",
    "root:{}\n",
    "term1:{}\n",
    "term2:{}\n",
    "term3:{}\n",
    "'''.format(boc_root, boc_term1, boc_term2, boc_term3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节将介绍以下五个度量:\n",
    "- 汉明距离(Hamming distance)\n",
    "- 曼哈顿距离(Manhattan distance)\n",
    "- 欧几里得距离(Euclidean distance)\n",
    "- 莱文斯坦编辑距离(Levenshtein edit distance)\n",
    "- 余弦距离(Cosine distance)和相似度\n",
    "\n",
    "首先,设置一些必要的变量以存储根词项,其他词项以及它们的向量化表示."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_term = root\n",
    "root_vector = vec_root\n",
    "root_boc_vector = boc_root\n",
    "\n",
    "term1 = [term1, term2, term3]\n",
    "vector_terms = [vec_term1, vec_term2, vec_term3]\n",
    "boc_vector_terms = [boc_term1, boc_term2, boc_term3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 汉明距离\n",
    "汉明距离是两个长度相等的字符串之间的测量距离.它的正式定义是两个长度相等的字符串之间互异字符或符号的位置的数量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hamming_distance(u,v,norm=False):\n",
    "    if u.shape!=v.shape:\n",
    "        raise ValueError('The vectors must have equal lengths.')\n",
    "        \n",
    "    return (u!=v).sum() if not norm else (u!=v).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance between root: Believe and term: Believe is 2\n",
      "Hamming distance between root: Believe and term: beleive is 6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The vectors must have equal lengths.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49584e160229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector_term\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     print 'Hamming distance between root: {} and term: {} is {}'.format(root_term, term, \n\u001b[0;32m----> 4\u001b[0;31m                                                                         hamming_distance(root_vector,vector_term,norm=False))\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0a8918851e5f>\u001b[0m in \u001b[0;36mhamming_distance\u001b[0;34m(u, v, norm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhamming_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The vectors must have equal lengths.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The vectors must have equal lengths."
     ]
    }
   ],
   "source": [
    "#compute Hamming distance\n",
    "for term,vector_term in zip(terms, vector_terms):\n",
    "    print 'Hamming distance between root: {} and term: {} is {}'.format(root_term, term, \n",
    "                                                                        hamming_distance(root_vector,vector_term,norm=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
