{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在第三章通过建立我们自己的分析器和标签器,你可以体会到机器学习模式是数据和算法的组合.\n",
    "\n",
    "机器学习的优势是一旦我们训练好了模型,就可以直接在新的以前未见到的数据上应用这个模型.\n",
    "\n",
    "NLP的一个最有挑战的问题是文本分类,它基于每个文件的内在属性或特性,尽量将文本文件划分到不同的类别.\n",
    "\n",
    "该技术可用在不同的领域,包括垃圾邮件识别和新闻分类.\n",
    "\n",
    "文本分类具有很多划分方法,本章主要介绍两种基于文档内容类型的分类:\n",
    "\n",
    "- 基于内容的分类\n",
    "    根据文本内容主题或题目的属性或权重来进行文档分类的.\n",
    "- 基于请求的分类\n",
    "    受到用户需求的影响,其目标是特定的用户群和读者.这类分类受到特殊策略和思想的控制\n",
    "    \n",
    "## 文本规范化处理\n",
    "\n",
    "定义一个规范化模块以处理文本文档规范化,并在后面建立分类器时使用这个处理模块.\n",
    "\n",
    "我们将在模块中实现和使用下面的规范化技术:\n",
    "\n",
    "- 扩展缩写词\n",
    "- 通过词形还原实现文本处理规范化\n",
    "- 去除特殊字符与符号\n",
    "- 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP #缩写映射\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english') #停用词\n",
    "wnl = WordNetLemmatizer() #词形还原\n",
    "\n",
    "def tokenize_text(text):\n",
    "    '''\n",
    "    实现词语切分,并去除分割后符号中的多余空格\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "                                if contraction_mapping.get(match) \\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\",\"\",expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    '''\n",
    "    词形还原函数\n",
    "    '''\n",
    "    #convert Penn treebank tag to wordnet tag\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startwith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startwith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startwith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startwith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "#lemmatize text based on POS tags\n",
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word,pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    '''\n",
    "    实现特殊符号和字符的去除\n",
    "    '''\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#文本处理流水线\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取\n",
    "下面将介绍和实现如下特征提取技术:\n",
    "\n",
    "- 词袋模型\n",
    "- TF-IDF模型\n",
    "- 高级词向量模型\n",
    "\n",
    "特征提取的实现可以分为两个模块."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型\n",
    "词袋模型是从文本文档中提取特征最简单但又最有效的技术.\n",
    "\n",
    "这个模型的本质是将文本文档转化成向量,这个向量表示在文档空间中全部不同的单词在该文档中出现的频率.\n",
    "\n",
    "该模型可以是n元分词词袋模型,它计算不同的n元分词在每个文档中的出现频率."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    '''\n",
    "    实现基于词袋的特征提取模块\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#build bow vectorizer and get features\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "\n",
    "features = bow_features.todense()\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract features from new document using built vectorizer\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print new_doc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'and', u'beautiful', u'blue', u'cheese', u'is', u'love', u'sky', u'so', u'the']\n"
     ]
    }
   ],
   "source": [
    "#print the features name\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features, columns=feature_names)\n",
    "    print df\n",
    "    \n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1]\n",
      " [1 1 1 0 1 1 0 0 2 1 1 0 0 0 2 2 0 0 0 0 0]\n",
      " [0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0]]\n",
      "[u'and', u'and sky', u'beautiful', u'beautiful sky', u'blue', u'blue and', u'blue cheese', u'cheese', u'is', u'is beautiful', u'is blue', u'is so', u'love', u'love blue', u'sky', u'sky is', u'so', u'so blue', u'the', u'the beautiful', u'the sky']\n"
     ]
    }
   ],
   "source": [
    "#build bow vectorizer and get features\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS,ngram_range=(1,2))\n",
    "\n",
    "features = bow_features.todense()\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "print features\n",
    "print feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF模型\n",
    "词袋模型存在一些问题,语料库全部文档中出现次数较多的单词将会拥有较高的频率,而这些并不是有意义的单词,\n",
    "\n",
    "而那些低频率的单词恰恰是能携带更多文档信息的单词.而TF-IDF可以解决这个问题.TF-IDF代表的是词频-逆文档频率,是两个度量的组合;\n",
    "\n",
    "**该技术最初是作为显示搜索引擎用户查询结果排序函数的一个度量,现在称为信息检索和文本特征提取的一部分**\n",
    "\n",
    "**词频**由tf表示,由词袋模型计算得出.任何文档的词频是该词在特定文档出现的原始频率值.\n",
    "\n",
    "**逆文档频率**由idf表示,是每个单词的文档频率的逆.该值由语料库中全部文档数量除以每个单词的文档频率,然后对结果应用对数运算变换其比例.(在这里需要对每个词频加1,意味着词汇表中每个单词至少在训练语料库中出现过一次,从而避免了除0的错误,称作**平滑逆文档频率**).\n",
    "\n",
    "$idf(t) = 1+\\log \\frac{C}{1+df(t)}$\n",
    "\n",
    "最终使用的tfidf是归一化后的值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2', smooth_idf=True, use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "#build tfidf transformer and show train corpus tfidf features\n",
    "tfidf_trans, tfidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tfidf_features.todense(),2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "#show tfidf features for new_doc using built tfidf transformer\n",
    "\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(),2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "#compute term frequency\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "\n",
    "#show term frequency\n",
    "display_features(tf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "#从词袋模型特征矩阵中获取DF(每个单词的文档频率)\n",
    "\n",
    "#build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df #to smoothen idf later\n",
    "\n",
    "#show document frequencies\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n",
      "[[ 1.92  0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.51  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    1.92  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    1.22  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    1.92  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    1.22  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    1.92  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    1.51]]\n"
     ]
    }
   ],
   "source": [
    "#compute inverse document frequencies\n",
    "total_docs = 1+len(CORPUS)\n",
    "idf = 1.0+np.log(float(total_docs)/df)\n",
    "\n",
    "#show inverse_document frequencies\n",
    "display_features([np.round(idf,2)], feature_names)\n",
    "\n",
    "#compute idf diagonal matrix\n",
    "total_features = bow_features.shape[1]\n",
    "\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features) #变成对角矩阵\n",
    "idf = idf_diag.todense()\n",
    "\n",
    "#print the idf diagonal matrix\n",
    "print np.round(idf, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "#compute tfidf feature matrix\n",
    "tfidf = tf*idf\n",
    "\n",
    "#show tfidf feature matrix\n",
    "display_features(np.round(tfidf,2),feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5   4.35  3.5   2.89]\n",
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "#tfidf归一化\n",
    "#compute L2 norm\n",
    "norms = norm(tfidf, axis=1)\n",
    "\n",
    "#print norms for each document\n",
    "print np.round(norms,2)\n",
    "\n",
    "#compute normalized tfidf\n",
    "norm_tfidf = tfidf/norms[:, None]\n",
    "\n",
    "#show final tfidf feature matrix\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "#compute new doc term freqs from bow freqs\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n",
    "\n",
    "#compute tfidf using idf matrix from train corpus\n",
    "nd_tfidf = nd_tf*idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]\n",
    "\n",
    "#show new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#直接从原始文档中计算文档基于tfidf的特征向量\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, norm='l2', smooth_idf=True, use_idf=True, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "#build tfidf vectorizer and get training corpus feature vectors\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(),2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "#get tfidf feature vector for the new document\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(),2), feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高级词向量模型\n",
    "word2vec模型由谷歌公司2013年发布,它是一个基于神经网络的实现,使用CBOW和skip-gram两种结构学习单词的分布式向量表示.\n",
    "\n",
    "gensim库里的word2vec的python实现基本思想是:提供一些文档语料作为输入,会得到词向量特征所谓输出.\n",
    "\n",
    "在模型内部,建立基于输入文档的**词汇表**,通过前面提到的各种技术学习**单词的向量表示**,一旦学习完成,\n",
    "\n",
    "就建立一个可用于从文档中提取单词向量的模型.使用如平均值和tfidf加权等方法,**使用词向量计算文档的平均向量**.\n",
    "\n",
    "在训练语料建立模型时,我们将主要关注下面的参数:\n",
    "- size:该参数用于设定词向量的维度,可以是几十到几千.可以尝试不同的维度,以获得最好的效果.\n",
    "- window: 该参数用于设定语境或窗口尺寸,指定了训练时对算法来说可算做上下文的单词窗口长度.\n",
    "- min_count: 该参数指定单词表中单词在语料中出现的最小次数.这个参数有助于去除一些文档中出现次数最少的不重要的单词\n",
    "- sample: 该参数用于对单词出现的频率进行下采样,其理想值在0.01到0.0001之间\n",
    "\n",
    "建立模型之后,将**基于一些加权策略来定义和实现两种词向量与文档结合的技术**.\n",
    "- 平均词向量\n",
    "- TF-IDF加权词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "#tokenize corpora\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "\n",
    "#build the word2vec model on our training corpus\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size=10, window=10, min_count=2, sample=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**开始实现特征提取技**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01608407 -0.04819566  0.04227461 -0.03011346  0.0254148   0.01728328\n",
      "  0.0155535   0.00774884 -0.02752112  0.01646519]\n",
      "[-0.0472235   0.01662185 -0.01221706 -0.04724348 -0.04384995  0.00193343\n",
      " -0.03163504 -0.03423524  0.02661656  0.03033725]\n"
     ]
    }
   ],
   "source": [
    "#平均词向量,上面的模型为单词表中每个单词创建一个单词表示,我们可以输入下面的代码来查看它们.\n",
    "print model['sky']\n",
    "\n",
    "print model['blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用平均词向量的方法计算文档向量\n",
    "import numpy as np\n",
    "#define function to average word vectors for a text document \n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype='float64')\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords+1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "#generalize above function for a corpus of documents\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.006 -0.01   0.015 -0.014  0.004 -0.006 -0.024 -0.007 -0.001 -0.   ]\n",
      " [-0.008 -0.01   0.021 -0.019 -0.002 -0.002 -0.011  0.002  0.003 -0.001]\n",
      " [-0.003 -0.007  0.008 -0.02  -0.001 -0.004 -0.014 -0.015  0.002 -0.01 ]\n",
      " [-0.047  0.017 -0.012 -0.047 -0.044  0.002 -0.032 -0.034  0.027  0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "#get averaged word vectors for our training CORPUS\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, model=model, num_features=10)\n",
    "print np.round(avg_word_vec_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.016 -0.016  0.015 -0.039 -0.009  0.01  -0.008 -0.013 -0.     0.023]]\n"
     ]
    }
   ],
   "source": [
    "#get averaged word vectors for our test new_doc\n",
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc, model=model, num_features=10)\n",
    "print np.round(nd_avg_word_vec_features,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF加权平均词向量**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to compute tfidf weighted averaged word vector for a document\n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] if tfidf_vocabulary.get(word) else 0 for word in words]\n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,), dtype='float64')\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word]*word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    \n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "#generalize above function for a corpus of documents\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, tfidf_vocabulary, model, num_features):\n",
    "    docs_tfidfs = [(doc, doc_tfidf) for doc, doc_tfidf in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary, model, num_features) \n",
    "               for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.011 -0.011  0.014 -0.011  0.007 -0.007 -0.024 -0.008 -0.004 -0.004]\n",
      " [-0.    -0.014  0.028 -0.014  0.004 -0.003 -0.012  0.011 -0.001 -0.002]\n",
      " [-0.001 -0.008  0.007 -0.019  0.001 -0.004 -0.012 -0.018  0.001 -0.014]\n",
      " [-0.047  0.017 -0.012 -0.047 -0.044  0.002 -0.032 -0.034  0.027  0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "#get tfidf weights and vocabulary from earlier results and compute result\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, tfidf_vectors=corpus_tfidf,\n",
    "                                                                    tfidf_vocabulary=vocab, model=model,num_features=10)\n",
    "print np.round(wt_tfidf_word_vec_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.012 -0.019  0.018 -0.038 -0.006  0.01  -0.006 -0.011 -0.003  0.023]]\n"
     ]
    }
   ],
   "source": [
    "#compute avgd word vector for test new_doc\n",
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc, tfidf_vectors=nd_tfidf,\n",
    "                                                                        tfidf_vocabulary=vocab,model=model, num_features=10)\n",
    "print np.round(nd_wt_tfidf_word_vec_features, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类算法\n",
    "分类算法是有监督的机器学习算法.分类算法整体有三个过程:\n",
    "- 训练是有监督学习算法分析和尝试推理训练数据的模式.\n",
    "- 评估包括测试模型的预测性能,检验它在训练数据集上训练和学习效果如何\n",
    "- 调优也称为超参数调优或优化.\n",
    "\n",
    "### 多项式朴素贝叶斯\n",
    "该算法是主流的朴素贝叶斯算法的一个特例,用于超过两类的预测和分类任务.朴素贝叶斯算法是一个将贝叶斯定义应用于实践的有监督学习算法.\n",
    "\n",
    "**它有一个\"朴素\"的假设条件:每个特征与其他特征之间相互独立.(特征之间的相互独立)**\n",
    "\n",
    "在给定特征X的条件下,y发生的概率,基于朴素假设条件,我们可以把这个概率公式表示成后验概率公式$后验概率=\\frac{步验X似然值}{证据}$,这里步验表示为P(y),似然值为$\\prod_{i=1}^{n}P(x_i|y)$,证据表示为P(X).\n",
    "\n",
    "从这个方程, 我们通过结合MAP(最大后验概率)决策规则就可以建立贝叶斯分类器,MAP代表最大后验概率.\n",
    "\n",
    "该方法在训练数据不充足时也能很好的工作,但是当出现多特征时,会导致**维度灾难**.\n",
    "\n",
    "朴素贝叶斯通过解耦类变量--相关条件的特征分布的方法解决这个问题,从而**使各分布作为单一维度的分布独立估计**.\n",
    "\n",
    "sklearn中的MultinomialNB类中提供了一个多项式朴素贝叶斯的优秀实现.\n",
    "\n",
    "### 评估分类模型\n",
    "分类模型的性能一般基于模型对新数据的预测输出结果.\n",
    "\n",
    "有多个指标可以判定模型的预测性能,但我们主要关注以下几个指标:\n",
    "- 准确率(Accuracy)\n",
    "- 精确率(precision)\n",
    "- 召回率(recall)\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual counts: [('ham', 10), ('spam', 10)]\n",
      "Predicted counts: [('spam', 11), ('ham', 9)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "actual_labels = ['spam','ham','spam','spam','spam',\n",
    "                'ham','ham','spam','ham','spam',\n",
    "                'spam','ham','ham','ham','spam',\n",
    "                'ham','ham','spam','spam','ham']\n",
    "predicted_labels = ['spam','spam','spam','ham','spam',\n",
    "                   'spam','ham','ham','spam','spam',\n",
    "                   'ham','ham','spam','ham','ham',\n",
    "                   'ham','spam','ham','spam','spam']\n",
    "ac = Counter(actual_labels)\n",
    "pc = Counter(predicted_labels)\n",
    "\n",
    "print 'Actual counts:', ac.most_common()\n",
    "print 'Predicted counts:', pc.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted:    \n",
      "                   spam ham\n",
      "Actual: spam          5   5\n",
      "        ham           6   4\n"
     ]
    }
   ],
   "source": [
    "#建立一个混淆矩阵\n",
    "cm = metrics.confusion_matrix(y_true=actual_labels, y_pred=predicted_labels,labels=['spam','ham'])\n",
    "print pd.DataFrame(data=cm, columns=pd.MultiIndex(levels=[['Predicted:'],['spam','ham']], labels=[[0,0],[0,1]]),\n",
    "                  index=pd.MultiIndex(levels=[['Actual:'],['spam','ham']], labels=[[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_class = 'spam'\n",
    "true_positive = 5.\n",
    "false_positive=6.\n",
    "false_negative=5.\n",
    "true_negative=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n",
      "Manually computed accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "#准确率计算\n",
    "accuracy = np.round(metrics.accuracy_score(y_true=actual_labels,y_pred=predicted_labels),2)\n",
    "accuracy_manual = np.round((true_positive+true_negative)/(true_positive+true_negative+false_negative+false_positive),2)\n",
    "print 'Accuracy:', accuracy\n",
    "print 'Manually computed accuracy:', accuracy_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.45\n",
      "Manually computed precision: 0.45\n"
     ]
    }
   ],
   "source": [
    "#精确率计算\n",
    "precision = np.round(metrics.precision_score(y_true=actual_labels,y_pred=predicted_labels,pos_label=positive_class),2)\n",
    "precision_manual = np.round((true_positive)/(true_positive+false_positive),2)\n",
    "print 'precision:', precision\n",
    "print 'Manually computed precision:', precision_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.5\n",
      "Manually computed recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "#召回率计算,召回率的定义是正类中被正确预测的实例数量.也被称为命中率,覆盖率或灵敏度\n",
    "recall = np.round(metrics.recall_score(y_true=actual_labels,y_pred=predicted_labels,pos_label=positive_class),2)\n",
    "recall_manual = np.round((true_positive)/(true_positive+false_negative),2)\n",
    "print 'recall:', recall\n",
    "print 'Manually computed recall:', recall_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.48\n",
      "Manually computed f1_score: 0.47\n"
     ]
    }
   ],
   "source": [
    "#F1 score是另一个准确性指标,通过计算准确率和召回率的调和平均值得到.\n",
    "f1_score = np.round(metrics.f1_score(y_true=actual_labels,y_pred=predicted_labels, pos_label=positive_class),2)\n",
    "f1_score_manual = np.round((2*precision*recall)/(precision+recall), 2)\n",
    "print 'F1 score:', f1_score\n",
    "print 'Manually computed f1_score:', f1_score_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立一个多分类系统\n",
    "- 数据集:使用sklearn下载的20个新闻组数据集.\n",
    "- 数据集组成:包括分散在20个不同类别或主题的18000个新闻组帖子,这就构建了20类分类问题.\n",
    "- 数据预处理:从文档中去除文件头,文件尾和引用,同时剔除空文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all', \n",
    "                             shuffle=True,\n",
    "                             remove=('headers','footers','quotes')) #获取新闻数据,掐头去尾去引用\n",
    "    return data\n",
    "\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3): #划分训练集和测试集\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=0.33, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "def remove_empty_docs(corpus, labels): #剔除空文档\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip(): #判断是否是空文档,即只包含空格的文档\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "            \n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#get the data\n",
    "dataset = get_data()\n",
    "print dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "Class label: 19\n",
      "Actual class label: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "#get corpus of documents and their corresponding labels\n",
    "corpus,labels = dataset.data, dataset.target\n",
    "corpus,labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "#see sample document and its label index, name\n",
    "print 'Sample document:', corpus[10]\n",
    "print 'Class label:', labels[10]\n",
    "print 'Actual class label:', dataset.target_names[labels[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare train and test datasets\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus, labels, test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对数据集进行规范化处理,就是词性还原,删除无效字符,删除停用词\n",
    "from normalization import normalize_corpus\n",
    "\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用前面建立好的特征提取模块从文档中提取特征.分别建立词袋模型,TF-IDF模型,平均词向量模型和TF-IDF加权平均词向量模型,并比较它们的性能\n",
    "\n",
    "#词袋模型\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "#TF-IDF模型\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "#文档分词\n",
    "tokenized_train = [nltk.word_tokenize(text) for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text) for text in norm_test_corpus]\n",
    "\n",
    "#建立word2vec模型\n",
    "model = gensim.models.Word2Vec(tokenized_train, size=500, window=100, min_count=30, sample=1e-3)\n",
    "\n",
    "#averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train, model=model, num_features=500)\n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test, model=model, num_features=500)\n",
    "\n",
    "#tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                                 tfidf_vectors=tfidf_train_features,\n",
    "                                                                 tfidf_vocabulary=vocab, model=model,\n",
    "                                                                 num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                                tfidf_vectors=tfidf_test_features,\n",
    "                                                                tfidf_vocabulary=vocab, model=model,\n",
    "                                                                num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#前面代码实现了从文本文档中提取了全部必要的特征之后,基于前面讨论的四个指标,我们定义一个函数用来评估分类模型\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print 'Accuracy:', np.round(metrics.accuracy_score(true_labels, predicted_labels),2)\n",
    "    print 'Precision:', np.round(metrics.precision_score(true_labels, predicted_labels,average='weighted'),2)\n",
    "    print 'Recall:', np.round(metrics.recall_score(true_labels, predicted_labels, average='weighted'), 2)\n",
    "    print 'F1 Score:', np.round(metrics.f1_score(true_labels,predicted_labels, average='weighted'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#开始训练分类模型\n",
    "def train_predict_evaluate_model(classifier, train_features, train_labels, test_features, test_labels):\n",
    "    #build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    #predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    #evaluate model prediciton performance\n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "Precision: 0.73\n",
      "Recall: 0.67\n",
      "F1 Score: 0.65\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Precision: 0.66\n",
      "Recall: 0.62\n",
      "F1 Score: 0.63\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.72\n",
      "Precision: 0.78\n",
      "Recall: 0.72\n",
      "F1 Score: 0.71\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.77\n",
      "F1 Score: 0.77\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Precision: 0.56\n",
      "Recall: 0.56\n",
      "F1 Score: 0.54\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Precision: 0.55\n",
      "Recall: 0.54\n",
      "F1 Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                  train_features=bow_train_features,\n",
    "                                                  train_labels=train_labels,\n",
    "                                                  test_features=bow_test_features,\n",
    "                                                  test_labels=test_labels)\n",
    "print '-'*50\n",
    "#support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                  train_features=bow_train_features,\n",
    "                                                  train_labels=train_labels,\n",
    "                                                  test_features=bow_test_features,\n",
    "                                                  test_labels=test_labels)\n",
    "print '-'*50\n",
    "#Multinomial Naive Bayes with tfidf features\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                    train_features=tfidf_train_features,\n",
    "                                                    train_labels=train_labels,\n",
    "                                                    test_features=tfidf_test_features,\n",
    "                                                    test_labels=test_labels)\n",
    "print '-'*50\n",
    "#support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                    train_features=tfidf_train_features,\n",
    "                                                    train_labels=train_labels,\n",
    "                                                    test_features=tfidf_test_features,\n",
    "                                                    test_labels=test_labels)\n",
    "print '-'*50\n",
    "#support vector machine with averaged word vector features\n",
    "svm_svgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                    train_features=avg_wv_train_features,\n",
    "                                                    train_labels=train_labels,\n",
    "                                                    test_features=avg_wv_test_features,\n",
    "                                                    test_labels=test_labels)\n",
    "print '-'*50\n",
    "#support vector machine with tfidf weighted averaged word vector features\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                      train_features=tfidf_wv_train_features,\n",
    "                                                      train_labels=train_labels,\n",
    "                                                      test_features=tfidf_wv_test_features,\n",
    "                                                      test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的输出结果来看,使用TF-IDF特征的SVM模型获得了最好的结果.\n",
    "\n",
    "我们可以建立svm tf-idf模型的混淆矩阵,以便了解模型性能不好的具体分类的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>225</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>224</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>226</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>269</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>248</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>251</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>277</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>213</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>270</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>266</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>288</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>164</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0   155    2    0    1    1    1    2    2    4    0    7    4    3    3    5   \n",
       "1     1  225    7    7    7   15    7    0    2    1    0    2    5    5    4   \n",
       "2     1   17  225   18    7   19    9    1    1    0    0    3    5    2    1   \n",
       "3     1    8   23  224   11    3   11    1    1    1    1    3    6    3    2   \n",
       "4     0    5    7   16  226    5    6    2    3    1    0    4    8    2    4   \n",
       "5     0   20   19    2    1  273    0    0    1    0    0    0    4    3    1   \n",
       "6     0    2    5   14   11    1  269   12    4    2    1    1    9    1    3   \n",
       "7     3    5    2    0    1    4    4  248   19    1    3    2    9    2    3   \n",
       "8     2    1    0    3    3    2    5   27  251    4    3    3    1    3    2   \n",
       "9     2    1    1    0    2    2    4    3    6  277   13    2    2    1    2   \n",
       "10    0    0    0    0    0    0    0    3    2    4  281    1    2    1    3   \n",
       "11    4    5    4    3    1    3    3    0    2    1    0  256    7    3    0   \n",
       "12    1    5    6   14    9    1   15   10    7    4    4    2  213    4    3   \n",
       "13    3    4    0    1    3    4    3    0    2    0    0    1    3  270    4   \n",
       "14    0    5    3    0    2    4    2    4    4    3    1    0    5    4  266   \n",
       "15   12    1    1    1    1    1    0    0    4    1    2    2    1    7    5   \n",
       "16    2    1    0    0    0    3    2    2    7    3    2   10    1    2    6   \n",
       "17    5    0    1    1    1    2    0    2    3    2    4    5    1    3    1   \n",
       "18   10    1    1    2    0    1    1    2    5    3    3    8    0    9    6   \n",
       "19   21    4    1    1    0    2    3    3    6    2    1    1    0   10    3   \n",
       "\n",
       "     15   16   17   18  19  \n",
       "0    33    4    9    8  19  \n",
       "1     2    4    0    3   0  \n",
       "2     2    1    1    1   0  \n",
       "3     0    1    0    0   0  \n",
       "4     1    1    0    1   0  \n",
       "5     0    0    1    0   0  \n",
       "6     1    1    1    1   0  \n",
       "7     0    4    4    2   0  \n",
       "8     3    1    3    5   0  \n",
       "9     3    2    1    1   0  \n",
       "10    1    0    1    2   2  \n",
       "11    1    6    2    5   0  \n",
       "12    1    1    0    1   1  \n",
       "13    3    3    0    4   0  \n",
       "14    2    4    1    3   1  \n",
       "15  288    5    4    2   5  \n",
       "16    1  229    5   12   3  \n",
       "17    5    5  262    9   3  \n",
       "18    5   31    7  164   3  \n",
       "19   61   21    8    5  60  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,20), columns=range(0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从混淆矩阵上可以看到,很多标签为0的文档被错误地分类到类标签15里面.\n",
    "\n",
    "同样对于类标签18的很多文档被错误地分类到类标签16里面.\n",
    "\n",
    "很多类标签标识19的文档被错误分类到类型标识为15里面."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism -> soc.religion.christian\n",
      "talk.politics.misc -> talk.politics.guns\n",
      "talk.religion.misc -> soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "class_names = dataset.target_names\n",
    "print class_names[0],'->',class_names[15]\n",
    "print class_names[18],'->', class_names[16]\n",
    "print class_names[19],'->',class_names[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从前面输出的可以发现,错误分类与实际分类存在相关性,导致它们的特征也存在相关性,所以会出现较高的误分类率.\n",
    "\n",
    "下面我们对误分类的情况进行分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "I would like a list of Bible contadictions from those of you who dispite being free from Christianity are well versed in the Bible. \n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  They spent quite a bit of time on the wording of the Constitution.  They picked words whose meanings implied the intent.  We have already looked in the dictionary to define the word.  Isn't this sufficient?   But we were discussing it in relation to the death penalty.  And, the Constitution need not define each of the words within.  Anyone who doesn't know what cruel is can look in the dictionary (and we did).\n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      " That's very interesting.    I wonder, are women's reactions recorded after a frustrating night with a man?   Is that considered to be important?\n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "Our Lord and Savior David Keresh has risen!   \tHe has been seen alive!   \tSpread the word!     --------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label==0 and predicted_label==15:\n",
    "        print 'Actual Label:', class_names[label]\n",
    "        print 'Predicted Label:', class_names[predicted_label]\n",
    "        print 'Document:-'\n",
    "        print re.sub('\\n',' ',document)\n",
    "        print\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "After the initial gun battle was over, they had 50 days to come out peacefully. They had their high priced lawyer, and judging by the posts here they had some public support. Can anyone come up with a rational explanation why the didn't come out (even after they negotiated coming out after the radio sermon) that doesn't include the Davidians wanting to commit suicide/murder/general mayhem?\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Yesterday, the FBI was saying that at least three of the bodies had gunshot wounds, indicating that they were shot trying to escape the fire.  Today's paper quotes the medical examiner as saying that there is no evidence of gunshot wounds in any of the recovered bodies.  At the beginning of this siege, it was reported that while Koresh had a class III (machine gun) license, today's paper quotes the government as saying, no, they didn't have a license.  Today's paper reports that a number of the bodies were found with shoulder weapons next to them, as if they had been using them while dying -- which doesn't sound like the sort of action I would expect from a suicide.  Our government lies, as it tries to cover over its incompetence and negligence.  Why should I believe the FBI's claims about anything else, when we can see that they are LYING?  This system of government is beyond reform.\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  Well, for one thing most, if not all the Dividians (depending on whether they could show they acted in self-defense and there were no illegal weapons), could have gone on with their life as they were living it. No one was forcing them to give up their religion or even their legal weapons. The Dividians had survived a change in leadership before so even if Koresch himself would have been convicted and sent to jail, they still could have carried on.   I don't think the Dividians were insane, but I don't see a reason for mass suicide (if the fire was intentional set by some of the Dividians.) We also don't know that, if the fire was intentionally set from inside, was it a generally know plan or was this something only an inner circle knew about, or was it something two or three felt they had to do with or without Koresch's knowledge/blessing, etc.? I don't know much about Masada. Were some people throwing others over? Did mothers jump over with their babies in their arms?\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "   True.  Today's Boston Globe interviewed a former Unification Church   leader who is now a consultant on cults.  He said the FBI's approach   was totally wrong.  He said they should have tried to break down the    BD's loyalty to Koresh through psychological means.   Koresh's whole   theology was based on an approaching confrontation with the forces    of evil in the world and a seige mentality based on this.  The Feds   played into his hands **PERFECTLY**.   By surrounding the compound   with tanks and playing loud rock music and glaring lights at them    they strongly reinforced Koresh's message that the outside world was   evil and threatening.    He said instead they should have set up    a picnic atmosphere, and acted inviting and friendly.  If they   broadcast anything over PA systems it should have been loving    relatives reflecting on pleasant events from the cult members'   childhoods.   The idea is to make the outside world and surrender   seem like a pleasant, desirable alternative.   Interesting comments. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "for document ,label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 18 and predicted_label==16:\n",
    "        print 'Actual Label:', class_names[label]\n",
    "        print 'Predicted Label:', class_names[predicted_label]\n",
    "        print 'Document:-'\n",
    "        print re.sub('\\n',' ',document)\n",
    "        print\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
